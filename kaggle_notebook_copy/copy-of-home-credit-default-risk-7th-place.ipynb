{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n[home credit default risk competition by home credit](https://www.kaggle.com/competitions/home-credit-default-risk/data)'s [7th place notebook](https://www.kaggle.com/code/jsaguiar/lightgbm-7th-place-solution/script) is made of pile of long definition of functions. This notebook follows that. But in Korean\n\n__another reference__ [홈크레딧 커널 스터디 by You Han Lee](https://www.youtube.com/watch?v=aoo1xrKQXFc&list=PLC_wC_PMBL5PexxXb6UC6pDXODE4xFWzL&index=3)\n\n# default risk: \n디폴트는 채무 불이행, 파산이라는 뜻이죠. 파산이라고 하면 되겠습니다. 먼저 오류가 조금 있는데 7th place notebook이 아니라 7th place용으로 ensemble을 여러 개 썼는데 그 모듈 일부를 수정해서 올린 거라고 합니다. 또 이건 notebook이 아니라 스크립트에요.\n\n따로 설명이 많지는 않습니다. [another similar version](https://github.com/js-aguiar/home-credit-default-competition) 이라는 링크가 있고, LightGBM의 boosting algorithm으로 goss를 썼다고 합니다. (다른 것으로는 gbdt가 있습니다) 데이터의 categorical feature를 label encode하는데 이게 앞의 구랑 무슨 관계인지 모르겠습니다.  \n\n__핵심 아이디어__ 는 시간 관련 feature를 추가하는 거라고 합니다. 마지막 대출신청, 마지막 x개월 동안(대출신청의) 합계 등? 테이블간의 비율 뿐만 아니라 대출 종류의 취합? 조금 애매합니다.\n\n## Import libraries\n잔뜩 불러옵니다. 낯선 것들을 꼽아보자면\n- gc : garbage collector\n- contextlib.contextmanager : resource 관리를 위한 데코레이터 contextmanager를 쓰기 위해 불러온 라이브러리입니다. 이것은 timer를 정의하고 with timer 구문을 쓰기 위해 씁니다. 이것은 파이썬 표준 라이브러리에 포함돼 있습니다.\n- multiprocessing : 표준 라이브러리입니다. mp.pool.Pool().map() 을 위해 씁니다.\n- functools.partial\n- scipy.stats.(kurtosis, iqr, skew)\n- sklearn.metrics.roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport re\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:18:54.349395Z","iopub.execute_input":"2022-10-10T10:18:54.349865Z","iopub.status.idle":"2022-10-10T10:18:54.358175Z","shell.execute_reply.started":"2022-10-10T10:18:54.349825Z","shell.execute_reply":"2022-10-10T10:18:54.356582Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# utility functions\n\n## get age label","metadata":{}},{"cell_type":"code","source":"def get_age_label(days_birth):\n    \"\"\" Return the age group label (int)\"\"\"\n    age_years = -days_birth /365\n    if age_years > 27: return 1\n    elif age_years > 40: return 2\n    elif age_years > 50: return 3\n    elif age_years > 65: return 4\n    elif age_years > 99: return 5\n    else: return 0","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:21.575652Z","iopub.execute_input":"2022-10-10T10:15:21.576183Z","iopub.status.idle":"2022-10-10T10:15:21.582520Z","shell.execute_reply.started":"2022-10-10T10:15:21.576137Z","shell.execute_reply":"2022-10-10T10:15:21.581434Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## 평균, 중앙값, 표준편차","metadata":{}},{"cell_type":"code","source":"def do_mean(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n    columns={counted: agg_name})\n    df=df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df\n\ndef do_median(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n    columns={counted: agg_name})\n    df=df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df\n\ndef do_std(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n    columns={counted: agg_name})\n    df=df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:23.084902Z","iopub.execute_input":"2022-10-10T10:15:23.085751Z","iopub.status.idle":"2022-10-10T10:15:23.096856Z","shell.execute_reply.started":"2022-10-10T10:15:23.085705Z","shell.execute_reply":"2022-10-10T10:15:23.095701Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def label_encoder(df, categorical_columns=None):\n    \"\"\"Encode categorical values as integers by pandas.factorize\"\"\"\n    if not categorical_columns:\n        #if 'categorical_columns' is not given, take 'object' data type columns as categorical columns\n        categorical_columns=[col for col in df.columns if df[col].dtype == 'object']\n    for col in categorical_columns:\n        df[col], uniques = pd.factorize(df[col])\n    return df, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:23.593870Z","iopub.execute_input":"2022-10-10T10:15:23.594284Z","iopub.status.idle":"2022-10-10T10:15:23.599795Z","shell.execute_reply.started":"2022-10-10T10:15:23.594245Z","shell.execute_reply":"2022-10-10T10:15:23.598997Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def drop_application_columns(df):\n    \"\"\"drop features based on permutation feature importance\"\"\"\n    drop_list=['CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE']\n    #drop most flag document columns\n    for doc_num in [2,4,5,6,7,9,10,11,12,13,14,15,16,17,19,20,21]:\n        drop_list.append('FLAG_DOCUMENT_{}'.format(doc_num))\n    df.drop(drop_list, axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:24.177301Z","iopub.execute_input":"2022-10-10T10:15:24.178487Z","iopub.status.idle":"2022-10-10T10:15:24.188088Z","shell.execute_reply.started":"2022-10-10T10:15:24.178436Z","shell.execute_reply":"2022-10-10T10:15:24.187216Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n    \"\"\"one-hot encode categorical columns by pandas.get_dummies\"\"\"\n    original_columns = list(df.columns)\n    if not categorical_columns:\n        categorical_columns=[col for col in df.columns if df[col].dtype=='object']\n    df=pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    categorical_columns=[c for c in df.columns if c not in original_columns]\n    return df, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:24.847980Z","iopub.execute_input":"2022-10-10T10:15:24.848401Z","iopub.status.idle":"2022-10-10T10:15:24.855836Z","shell.execute_reply.started":"2022-10-10T10:15:24.848366Z","shell.execute_reply":"2022-10-10T10:15:24.854469Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by=aggregate_by)\n    return df_to_merge.merge(agg_df, how='left', on=aggregate_by)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:25.412739Z","iopub.execute_input":"2022-10-10T10:15:25.413256Z","iopub.status.idle":"2022-10-10T10:15:25.419470Z","shell.execute_reply.started":"2022-10-10T10:15:25.413222Z","shell.execute_reply":"2022-10-10T10:15:25.417997Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by = 'SK_ID_CURR'):\n    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n                              for e in agg_df.columns.tolist()])\n    return agg_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:25.974489Z","iopub.execute_input":"2022-10-10T10:15:25.975239Z","iopub.status.idle":"2022-10-10T10:15:25.983906Z","shell.execute_reply.started":"2022-10-10T10:15:25.975183Z","shell.execute_reply":"2022-10-10T10:15:25.982701Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def do_sum(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp; gc.collect()\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:26.814655Z","iopub.execute_input":"2022-10-10T10:15:26.815055Z","iopub.status.idle":"2022-10-10T10:15:26.846953Z","shell.execute_reply.started":"2022-10-10T10:15:26.815021Z","shell.execute_reply":"2022-10-10T10:15:26.845688Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### add_features 는 아무데서도 쓰이지 않습니다.\n\n```python\ndef add_features(feature_name, aggs, features, feature_names, groupby): #it is never called\n    #feature_name is dictionary type\n    feature_names.extend(['{}_{}'.format(feature_name, agg) for agg in aggs])\n    \n    for agg in aggs:\n        #kurtosis and iqr are imported from scipy.stats\n        if agg == 'kurt':\n        elif agg == 'iqr':\n            agg_func = iqr\n        else:\n            agg_func = agg\n        \n            agg_func = kurtosis \n        g = groupby[feature_name].agg(agg_func).\n```","metadata":{}},{"cell_type":"code","source":"def add_features_in_group(features, gr_, feature_name, aggs, prefix):\n    \"\"\"\n    add_features_in_group(features, gr_, feature_name, aggs, prefix)\n    add some features to the 'dictionary', applying functions on column of dataframe.\n    features: dictionary\n    gr_: (conditionally filtered) dataframe\n    feature_name, prefix : string\n    aggs: list of string\n    \"\"\"\n    for agg in aggs:\n        if agg == 'sum':\n            features['{}{}_sum'.format(prefix, feature_name)] = gr_[feature_name].sum()\n        elif agg == 'mean':\n            features['{}{}_mean'.format(prefix, feature_name)] = gr_[feature_name].mean()\n        elif agg == 'max':\n            features['{}{}_max'.format(prefix, feature_name)] = gr_[feature_name].max()\n        elif agg == 'min':\n            features['{}{}_min'.format(prefix, feature_name)] = gr_[feature_name].min()\n        elif agg == 'std':\n            features['{}{}_std'.format(prefix, feature_name)] = gr_[feature_name].std()\n        elif agg == 'count':\n            features['{}{}_count'.format(prefix, feature_name)] = gr_[feature_name].count()\n        elif agg == 'skew':\n            features['{}{}_skew'.format(prefix, feature_name)] = skew(gr_[feature_name])\n        elif agg == 'kurt':\n            features['{}{}_kurt'.format(prefix, feature_name)] = kurt(gr_[feature_name])\n        elif agg == 'iqr':\n            features['{}{}_iqr'.format(prefix, feature_name)] = iqr(gr_[feature_name])\n        elif agg == 'median':\n            features['{}{}_median'.format(prefix, feature_name)] = gr_[feature_name].median()\n    return features\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_trend_feature(features, gr, feature_name, prefix):\n    \"\"\"\n    add_trend_feature(features, gr, feature_name, prefix)\n    try linear regression on gr[feature_name] and add trend(gradient) to the dictionary and return,\n        where x axis is [[0],[1],[2]....]\n    features: dictionary\n    gr: conditionally filltered dataframe\n    \"\"\"\n    y = gr[feature_name].values\n    try:\n        x = np.arange(0, len(y)).reshape(-1, 1)\n        lr = LinearRegression()\n        lr.fit(x, y)\n        trend = lr.coef_[0]\n    except:\n        trend = np.nan\n    features['{}{}'.format(prefix, feature_name)] = trend\n    return features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parallel_apply(groups, func, index_name='Index', num_workers=0, chunk_size=100_000):\n    \"\"\" \n    multiprocessing.pool.Pool().map() is parallel equivalent of map() (built in function).\n    see\n    https://docs.python.org/ko/3/library/multiprocessing.html#multiprocessing.pool.Pool\n    for details.\"\"\"\n    if num_workers <= 0: num_workers = NUM_THREADS\n    #n_chunks = np.ceil(1.0 * groups.ngroups/chunk_size) #?\n    indices, features = [], []\n    for index_chunk, group_chunk in chunk_groups(groups, chunk_size):\n        with mp.pool.Pool(num_workers) as executor:\n            features_chunk = executor.map(func, group_chunk)\n        features.extend(features_chunk)\n        indeces.extend(index_chunk)\n    \n    features = pd.DataFrame(features)\n    features.index = indeces\n    features.index.name = index_name\n    return features\n        \ndef chunk_groups(groupby_object, chunk_size):\n    \"\"\"yield sub-dataframe divided by groupby.\"\"\"\n    n_groups = groupby_object.ngroups\n    group_chunk, index_chunk = [], []\n    for i, (index, df) in enumerate(groupby_object):\n        group_chunk.append(df)\n        group_index.append(index)\n        if (i+1) % chunk_size == 0 or i + 1 ==n_groups:\n            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n            group_chunk, index_chunk = [], []\n            yield index_chunk_, group_chunk_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory(df):\n    \"\"\"reduce memory usage of a dataframe by setting data types\"\"\"\n    start_mem = df.memory_usage().sum() / 1024 **2\n    #1024**2 byte = 1 mega byte\n    print(\"Initial df memory usage: {:.2f} MB for {} columns\"\n          .format(start_mem, len(df.columns)))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            cmin = df[col].min()\n            cmax = df[col].max()\n            if str(col_type[:3])== 'int':\n                #can use unsigned int here too\n                if cmin > np.iinfo(np.int8).min and cmax<np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax<np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax<np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax<np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax<np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax<np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                df[col] = df[col].astype(np.float64)\n    end_mem =df.memory_usage().sum() / 1024**2\n    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n    print('Final memory usage is {:.2f} MB - {:.1f}% decreased'.format(end_mem, memory_reduction))\n    return df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 구문 해석\n하나하나 살펴봅시다. 먼저 실행되는 순서를 살펴봅니다. 함수 정의를 다 건너뛰고 보면 'configuration'이 나옵니다. 이것 역시 변수의 할당일 뿐 실행은 아닙니다. \n\n그러면 1083-1087행이 남습니다.\n\n```python\n#1083 - 1087 rows\nif __name__ == \"__main__\":\n    pd.set_option('display.max_rows', 60)\n    pd.set_option('display.max_columns', 100)\n    with timer('Pipeline total time\"):\n              main(debug = False)\n```\n먼저 \\_\\_name\\_\\_ 은 사용자가 실행시킨 스크립트(.py)파일에서 직접 호출되면 \\_\\_main\\_\\_ 이란 값을, 그 외의 모듈에서 실행되면 모듈의 이름을 돌려주는 사전 정의된 변수라고 합니다. 밑줄 두개로 둘러쌓인 built in method들을 magic method라고 하고, 개중에는 \\_\\_init\\_\\_ 이게 제일 익숙할 것 같습니다.\n호출순서대로는 timer가 먼저 등장합니다.\n\n## timer\n이 함수의 정의를 살펴보면 contextmanager 데코레이터가 붙어있습니다. with a as b:, yield 이랑 같이 가는 거라고 보면 되겠습니다. 자세한 설명은 [python 공식문서](https://docs.python.org/ko/3/library/contextlib.html) 나 [도장.io](https://dojang.io/mod/page/view.php?id=2412) 를 참고하는게 좋겠습니다. 도장io 사이트는 그냥 익숙한 사이트라서 봅니다.\n\n\n","metadata":{}},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(name, time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:29.423485Z","iopub.execute_input":"2022-10-10T10:15:29.423921Z","iopub.status.idle":"2022-10-10T10:15:29.429941Z","shell.execute_reply.started":"2022-10-10T10:15:29.423862Z","shell.execute_reply":"2022-10-10T10:15:29.428779Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## main function\n그 다음으로는 main(debug=False)로 main function을 실행하는데 이걸 봐야겠네요.","metadata":{}},{"cell_type":"code","source":"def main(debug = False):\n    num_rows = 30000 if debug else None\n    #load data, merge, and clean memory\n    with timer(\"application_train and application_test\"):\n        df = get_train_test(DATA_DIRECTORY, num_rows=num_rows)\n        print(\"Application dataframe shape: \", df.shape)\n    with timer(\"Bureau and bureau_balancce data\"):\n        bureau_df=get_bureau(DATA_DIRECTORY, num_rows=num_rows)\n        df = pd.merge(df, bureau_df, on='SK_ID_CURR', how='left')\n\n        print(\"Bureau dataframe shape: \", bureau_df.shape)\n        del bureau_df; gc.collect()\n    with timer(\"previous_application\"):\n        prev_df = get_previous_applications(DATA_DIRECTORY, num_rows)\n        df=pd.merge(df, prev_df, on='SK_ID_CURR', how='left')\n        del prev_df; gc.collect()\n    with timer(\"previous applications balance\"):\n        pos=get_pos_cash(DATA_DIRECTORY, num_rows)\n        df=pd.merge(df, pos, on='SK_ID_CURR', how='left')\n        print(\"Pos-cash dataframe shape: \", pos.shape)\n        del pos; gc.collect()\n        ins = get_installment_payments(DATA_DIRECTORY, num_rows)\n        df=pd.merge(df, ins, on='SK_ID_CURR', how='left')\n        print(\"Installments dataframe shape: \", ins.shape)\n        del ins; gc.collect()\n        cc=get_credit_card(DATA_DIRECTORY, num_rows)\n        df=pd.merge(df, cc, on='SK_ID_CURR', how='left')\n        print(\"Credit card dataframe shape: \", cc.shape)\n        del cc; gc.collect()\n    #add ratios and groupby between different tables\n    df=add_ratios_features(df)\n    df=reduce_memory(df)\n    lgbm_categorical_feat=[\n        'CODE_GENDER', 'FLAG_OWN_CAR','NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n        'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE',\n        'ORGANIZATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_TYPE_SUITE', 'WALLSMATERIAL_MODE']\n    with timer(\"Run LightGBM\"):\n        feat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n        print(feat_importance)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T09:51:49.155872Z","iopub.execute_input":"2022-10-10T09:51:49.156808Z","iopub.status.idle":"2022-10-10T09:51:49.169092Z","shell.execute_reply.started":"2022-10-10T09:51:49.156768Z","shell.execute_reply":"2022-10-10T09:51:49.167646Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def add_ratios_features(df):\n    #credit to income ratio:\n    df['BUREAU_INCOME_CREDIT_RATIO']=df['BUREAU_AMT_CREDIT_SUM_MEAN'] / df['AMT_INCOME_TOTAL']\n    df['BUREAU_ACTIVE_CREDIT_TO_INCOME_RATIO']=df['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_INCOME_TOTAL']\n    #previous to current credit ratio\n    df['CURRENT_TO_APPROVED_CREDIT_MIN_RATIO']=df['APPROVED_AMT_CREDIT_MIN']/df['AMT_CREDIT']\n    df['CURRENT_TO_APPROVED_CREDIT_MAX_RATIO']=df['APPROVED_AMT_CREDIT_MAX']/df['AMT_CREDIT']\n    df['CURRENT_TO_APPROVED_CREDIT_MEAN_RATIO']=df['APPROVED_AMT_CREDIT_MEAN']/df['AMT_CREDIT']\n    #PREVIOUS TO CURRENT ANNUITY RATIO\n    df['CURRENT_TO_APPROVED_ANNUITY_MAX_RATIO']=df['APPROVED_AMT_ANNUITY_MAX']/df['AMT_ANNUITY']\n    df['CURREMT_TO_APPROVED_ANNUITY_MEAN_RATIO']=df['APPROVED_AMT_ANNUITY_MEAN']/df['AMT_ANNUITY']\n    df['PAYMENT_MIN_TO_ANNUITY_RATIO']=df['INS_AMT_PAYMENT_MIN']/df['AMT_ANNUITY']\n    df['PAYMENT_MAX_TO_ANNUITY_RATIO']=df['INS_AMT_PAYMENT_MAX']/df['AMT_ANNUITY']\n    df['PAYMENT_MEAN_TO_ANNUITY_RATIO']=df['INS_AMT_PAYMENT_MEAN']/df['AMT_ANNUITY']\n    #PREVIOUS TO CURRENT CREDIT TO ANNUITY RATIO\n    df['CTA_CREDIT_TO_ANNUITY_MAX_RATIO']=df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MAX']/df['CREDIT_TO_ANNUITY_RATIO']\n    df['CTA_CREDIT_TO_ANNUITY_MEAN_RATIO']=df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MEAN']/df['CREDIT_TO_ANNUITY_RATIO']\n    #DAYS DIFFERENCES AND RATIOS\n    df['DAYS_DECISION_MEAN_TO_BIRTH']=df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_BIRTH']\n    df['DAYS_CREDIT_MEAN_TO_BIRTH']=df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_BIRTH']\n    df['DAYS_DECISION_MEAN_TO_EMPLOYED']=df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_EMPLOYED']\n    df['DAYS_CREDIT_MEAN_TO_EMPLOYED']=df['BUREAU_DAYS_CREDIT_MEAN']/df['DAYS_EMPLOYED']\n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:41.019899Z","iopub.execute_input":"2022-10-10T10:15:41.020318Z","iopub.status.idle":"2022-10-10T10:15:41.032010Z","shell.execute_reply.started":"2022-10-10T10:15:41.020284Z","shell.execute_reply":"2022-10-10T10:15:41.030665Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM Model","metadata":{}},{"cell_type":"code","source":"def kfold_lightgbm_sklearn(data, categorical_feature = None):\n    data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_\\s]+', '', x)) #added line to fix character error\n\n    df=data[data['TARGET'].notnull()]\n    test= data[data['TARGET'].isnull()]\n    print(\"Train/valid shape: {}, test_shape: {}\".format(df.shape, test.shape))\n    del_features = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n    predictors = list(filter(lambda v: v not in del_features, df.columns))\n\n    \n    \n    if not STRATIFIED_KFOLD:\n        folds=KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n    else:\n        folds=StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n    \n    #Hold oof predictions, test predictions, feature importance and training/valid auc\n    oof_preds = np.zeros(df.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    importance_df=pd.DataFrame()\n    eval_results=dict()\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['TARGET'])):\n        train_x, train_y = df[predictors].iloc[train_idx], df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = df[predictors].iloc[valid_idx], df['TARGET'].iloc[valid_idx]\n        \n        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n        \n        if not categorical_feature:\n            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                   eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING)\n        else:\n            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                   eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING,\n                   feature_name=list(df[predictors].columns), categorical_feature=categorical_feature)\n        oof_preds[valid_idx]=clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:,1]\n        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:,1]/folds.n_splits\n        \n        #feature importance by gain and split\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"]=predictors\n        fold_importance[\"gain\"]=clf.booster_.feature_importance(importance_type='gain')\n        fold_importance[\"split\"]=clf.booster_.feature_importance(importance_type='split')\n        importance_df=pd.concat([importance_df, fold_importance], axis=0)\n        eval_results['train_{}'.format(n_fold+1)] = clf.evals_result_['training']['auc']\n        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n        \n        print('FOLD %2d AUC : %.6f' % (n_fold+1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n        \n    print('Full AUC score %.6f' % roc_auc_score(df['TARGET'], oof_preds))\n    test['TARGET']=sub_preds.copy()\n    \n    #get the average feature importance between folds\n    mean_importance = importance_df.groupby('feature').mean().reset_index()\n    mean_importance.sort_values(by='gain', ascending=False, inplace=True)\n    #save feature importance, test predictions and oof predictions as csv\n    if GENERATE_SUBMISSION_FILES:\n        \n        #generative oof csv\n        oof=pd.DataFrame()\n        oof['SK_ID_CURR']=df['SK_ID_CURR'].copy()\n        df['PREDICTION']=oof_preds.copy()\n        df['TARGET']=df['TARGET'].copy()\n        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n        #save submission (test data) and feature importance\n        test[['SK_ID_CURR', 'TARGET']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    return mean_importance\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:30:40.621796Z","iopub.execute_input":"2022-10-10T10:30:40.622201Z","iopub.status.idle":"2022-10-10T10:30:40.645744Z","shell.execute_reply.started":"2022-10-10T10:30:40.622164Z","shell.execute_reply":"2022-10-10T10:30:40.644687Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"# get_train_test","metadata":{}},{"cell_type":"code","source":"def get_train_test(path, num_rows = None):\n    \"\"\"process application_train.csv and application_test.csv and return a pandas dataframe\"\"\"\n    train = pd.read_csv(os.path.join(path, 'application_train.csv'), nrows=num_rows)\n    test = pd.read_csv(os.path.join(path, 'application_test.csv'), nrows=num_rows)\n    df=train.append(test)\n    del train, test; gc.collect()\n    #data cleaning\n    df = df[df['CODE_GENDER'] != 'XNA'] #4 people with XNA code gender\n    df = df[df['AMT_INCOME_TOTAL']<20_000_000]\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n    \n    #Flag_document features - count and kurtosis\n    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n    df['DOCUMENT_COUNT']=df[docs].sum(axis=1)\n    df['NEW_DOC_KURT']=df[docs].kurtosis(axis=1)\n    #categorical age - based on target=1 plot\n    df['AGE_RANGE']=df['DAYS_BIRTH'].apply(get_age_label)\n    \n    #new features based on External sources\n    df['EXT_SOURCES_PROD']=df['EXT_SOURCE_1']*df['EXT_SOURCE_2']*df['EXT_SOURCE_3']\n    df['EXT_SOURCES_WEIGHTED']=df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 + df.EXT_SOURCE_3 * 3\n    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n        df[feature_name]=eval('np.{}'.format(function_name))(\n        df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n    \n    #credit ratios\n    df['CREDIT_TO_ANNUITY_RATIO']=df['AMT_CREDIT']/df['AMT_ANNUITY']\n    df['CREDIT_TO_GOODS_RATIO']=df['AMT_CREDIT']/df['AMT_GOODS_PRICE']\n    #income ratios\n    df['ANNUITY_TO_INCOME_RATIO']=df['AMT_ANNUITY']/df['AMT_INCOME_TOTAL']\n    df['CREDIT_TO_INCOME_RATIO']=df['AMT_CREDIT']/df['AMT_INCOME_TOTAL']\n    df['INCOME_EMPLOYED_RATIO']=df['AMT_INCOME_TOTAL']/df['DAYS_EMPLOYED']\n    df['PHONE_TO_BIRTH_RATIO']=df['AMT_INCOME_TOTAL']/df['DAYS_BIRTH']\n    #time_ratios\n    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED']/df['DAYS_BIRTH']\n    df['ID_TO_BIRTH_RATIO']=df['DAYS_ID_PUBLISH']/df['DAYS_BIRTH']\n    df['CAR_TO_BIRTH_RATIO']=df['OWN_CAR_AGE']/df['DAYS_BIRTH']\n    df['CAR_TO_EMPLOYED_RATIO']=df['OWN_CAR_AGE']/df['DAYS_EMPLOYED']\n    df['PHONE_TO_BIRTH_RATIO']=df['DAYS_LAST_PHONE_CHANGE']/df['DAYS_BIRTH']\n\n    #GroupyBy : Statistics for applications in the same group\n    group=['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n    \n    #below is copied- will be deleted-- \n    # Groupby: Statistics for applications in the same group\n    group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n\n    # Encode categorical features (LabelEncoder)\n    df, le_encoded_cols = label_encoder(df, None)\n    df = drop_application_columns(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:55.143812Z","iopub.execute_input":"2022-10-10T10:15:55.144669Z","iopub.status.idle":"2022-10-10T10:15:55.163969Z","shell.execute_reply.started":"2022-10-10T10:15:55.144598Z","shell.execute_reply":"2022-10-10T10:15:55.162682Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# GET_BUREAU\nBUREAU라는 다른 회사 데이터.","metadata":{}},{"cell_type":"code","source":"def get_bureau(path, num_rows=None):\n    \"\"\"load bureau.csv and bureau_balance.csv, process, return pandas dataframe\"\"\"\n    bureau=pd.read_csv(os.path.join(path, 'bureau.csv'), nrows=num_rows)\n    #credit duration and credit/account end date difference\n    bureau['CREDIT_DURATION']=-bureau['DAYS_CREDIT']+bureau['DAYS_CREDIT_ENDDATE']\n    bureau['ENDDATE_DIF']=bureau['DAYS_CREDIT_ENDDATE']-bureau['DAYS_ENDDATE_FACT']\n    #credit to debt ratio and difference\n    bureau['DEBT_PERCENTAGE']=bureau['AMT_CREDIT_SUM']/bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['DEBT_CREDIT_DIFF']=bureau['AMT_CREDIT_SUM']-bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['CREDIT_TO_ANNUITY_RATIO']=bureau['AMT_CREDIT_SUM']/bureau['AMT_ANNUITY']\n    \n    #one-hot encoder\n    bureau, categorical_cols=one_hot_encoder(bureau, nan_as_category=False)\n    #join bureau balance features\n    bureau = bureau.merge(get_bureau_balance(path, num_rows), how='left', on='SK_ID_BUREAU')\n    \n    #Flag months with late payments (days past due)\n    bureau['STATUS_12345']=0\n    for i in range(1,6):\n        bureau['STATUS_12345']+=bureau['STATUS_{}'.format(i)]\n    \n    # aggregate by number of months in balance and merge with bureau (loan length agg)\n    features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM',\n        'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n    agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n    agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n    bureau = bureau.merge(agg_length, how='left', on='MONTHS_BALANCE_SIZE')\n    del agg_length; gc.collect()\n    \n    #General loans aggregations\n    agg_bureau = group(bureau, 'BUREAU_', BUREAU_AGG)\n    #Active and closed loans aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    agg_bureau = group_and_merge(active, agg_bureau, 'BUREAU_ACTIVE_', BUREAU_ACTIVE_AGG)\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    agg_bureau = group_and_merge(closed, agg_bureau, 'BUREAU_CLOSED_', BUREAU_CLOSED_AGG)\n    del active, closed; gc.collect()\n    # Aggregations for the main loan types\n    for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n        type_df = bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n        prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n        agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_LOAN_TYPE_AGG)\n        del type_df; gc.collect()\n    #Time based aggregations\n    for time_frame in [6, 12]:\n        prefix = 'BUREAU_LAST{}M_'.format(time_frame)\n        time_frame_df = bureau[bureau['DAYS_CREDIT'] >= -30*time_frame]\n        agg_bureau = group_and_merge(time_frame_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n        del time_frame_df; gc.collect()\n    \n    # Last loan max overdue\n    sort_bureau = bureau.sort_values(by=['DAYS_CREDIT'])\n    gr = sort_bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].last().reset_index()\n    gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)\n    agg_bureau = agg_bureau.merge(gr, on='SK_ID_CURR', how='left')\n    # total debt/total credit and active loans debt / active loans credit\n    agg_bureau['BUREAU_DEPT_OVER_CREDIT'] = \\\n        agg_bureau['BUREAU_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_AMT_CREDIT_SUM_SUM']\n    agg_bureau['BUREAU_ACTIVE_DEBT_OVER_CREDIT'] = \\\n        agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM']\n    return agg_bureau","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:15:58.898492Z","iopub.execute_input":"2022-10-10T10:15:58.898884Z","iopub.status.idle":"2022-10-10T10:15:58.915467Z","shell.execute_reply.started":"2022-10-10T10:15:58.898853Z","shell.execute_reply":"2022-10-10T10:15:58.914678Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## GET_BUREAU_BALANCE","metadata":{}},{"cell_type":"code","source":"def get_bureau_balance(path, num_rows=None):\n    bb = pd.read_csv(os.path.join(path, 'bureau_balance.csv'), nrows=num_rows)\n    bb, categorical_cols = one_hot_encoder(bb, nan_as_category = False)\n    # rate for each category with decay?\n    bb_processed = bb.groupby('SK_ID_BUREAU')[categorical_cols].mean().reset_index()\n    # min, max, count and mean duration of payments (months)\n    agg = {'MONTHS_BALANCE':['min', 'max', 'mean', 'size']}\n    bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n    del bb; gc.collect()\n    return bb_processed","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:00.469014Z","iopub.execute_input":"2022-10-10T10:16:00.469431Z","iopub.status.idle":"2022-10-10T10:16:00.477060Z","shell.execute_reply.started":"2022-10-10T10:16:00.469396Z","shell.execute_reply":"2022-10-10T10:16:00.475703Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#def get_previous_applications()\n\ndef get_previous_applications(path, num_rows = None):\n    \"\"\"Process previous_application.csv and return a pandas DataFrame\"\"\"\n    prev=pd.read_csv(os.path.join(path, 'previous_application.csv'), nrows=num_rows)\n    pay=pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows=num_rows)\n    \n    # one - hot encode most important categorical features\n    ohe_columns=[\n        'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n    prev, categorical_cols = one_hot_encoder(prev, ohe_columns, nan_as_category = False)\n    \n    #add Features: ratios and differences\n    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT']/prev['AMT_ANNUITY']\n    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n    #interest ratio on previous application (simplified)\n    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    prev['SIMPLE_INTERESTS'] = (total_payment/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n    \n    #Active loans: approved, not complete\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n    \n    #Find how much was aleardy payed in active loans (using installments csv)\n    active_pay = pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]\n    active_pay_agg = active_pay.groupby('SK_ID_PREV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n    active_pay_agg.reset_index(inplace = True)\n    #active loans: difference between payed and installments\n    active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - active_pay_agg['AMT_PAYMENT']\n    #merge with active_df\n    active_df = active_df.merge(active_pay_agg, on= 'SK_ID_PREV', how='left')\n    active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - active_df['AMT_PAYMENT']\n    active_df['REPAYMENT_RATIO'] = active_df['AMT_PAYMENT'] / active_df['AMT_CREDIT']\n    #perform aggregations for active applications\n    active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n    active_agg_df(['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREV_ACTIVE_AMT_PAYMENT_SUM']/\\\n                 active_agg_df['PREV_ACTIVE_AMT_CREDIT_SUM'])\n    del active_pay, active_pay_agg, active_df; gc.collect()\n                  \n    #change 365243 to nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n    #Days last due difference (scheduled x done)\n    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n    approved['DAYS_LAST_DUE_DIFF'] = approved['DAYS_LAST_DUE_1ST_VERSION'] - approved['DAYS_LAST_DUE']\n    \n    #categorical features\n    categorical_agg = {key: ['mean'] for key in categorical_cols}\n    #perform general aggregations\n    agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})\n    #Merge active loans df on agg_prev\n    agg_prev = agg_prev.merge(active_agg_df, how='left', on='SK_ID_CURR')\n    del active_agg_df; gc.collect()\n    #aggregations for approved and refused loans\n    agg=prev = group_and_merge(approved, agg_prev, 'APPROVED_', PREVIOUS_APPROVED_AGG)\n    refuesd = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    agg_prev = group_and_merge(refused, agg_prev, 'REFUSED_', PREVIOUS_REFUSED_AGG)\n    del approved, refused; gc.collect()\n    #aggregations for consumer loans and cash loans\n    for loan_type in ['Consumer loans', 'Cash loans']:\n        type_df = prev[prev['NAME_CONTRACT_TYPE_{}'.format(loan_type)] == 1]\n        prefix = 'PREV_' + loan_type.split(\" \")[0] + '_'\n        agg_prev = group_and_merge(type_df, agg_prev, prefix, PREVIOUS_LOAN_TYPE_AGG)\n        del type_df; gc.collect()\n    \n    #get the SK_ID_PREV for loans with late payments (days past due: DPD)\n    pay['LATE_PAYMENT'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n    pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(lambda x: 1 if x > 0 else 0)\n    dpd_id = pay[pay['LATE_PAYMENT'] > 0]['SK_ID_PREV'].unique()\n    #Aggregations for loans with late payment\n    agg_dpd=group_and_merge(prev[prev['SK_ID_PREV'].isin(dpd_id)], agg_prev,\n                            'PREV_LATE_', PREVIOUS_LATE_PAYMENTS_AGG)\n    del agg_dpd, dpd_id; gc.collect()\n    #Aggregations for loans in the last x months\n    for time_frame in [12, 24]:\n        time_frame_df = prev[prev['DAYS_DECISION'] >= -30*time_frame]\n        prefix = 'PREV_LAST{}M_'.format(time_frame)\n        agg_prev = group_and_merge(time_frame_df, agg_prev, prefix, PREVIOUS_TIME_AGG)\n        del time_frame_df; gc.collect()\n    del prev; gc.collect()\n    return agg_prev\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T09:52:00.224878Z","iopub.execute_input":"2022-10-10T09:52:00.225733Z","iopub.status.idle":"2022-10-10T09:52:00.248722Z","shell.execute_reply.started":"2022-10-10T09:52:00.225687Z","shell.execute_reply":"2022-10-10T09:52:00.247404Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# GET_POS_CASH... etc","metadata":{}},{"cell_type":"code","source":"def get_pos_cash(path, num_rows = None):\n    \"\"\"Process POS_CASH_balance.csv and return pandas dataframe\"\"\"\n    pos = pd.read_csv(os.path.join(path, 'POS_CASH_balance.csv'), nrows = num_rows)\n    pos, categorical_cols = one_hot_encoder(pos, nan_as_category=False)\n    #Flag months with late payment\n    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    #aggregate by SK_ID_CURR\n    categorical_agg = {key: ['mean'] for key in categorical_cols}\n    pos_agg = group(pos, 'POS_', {**POS_CASH_AGG, *categorical_agg})\n    #sort and group by SK_ID_PREV\n    sort_pos = pos.sort_values(by = ['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.groupby('SK_ID_PREV')\n    df = pd.DataFrame()\n    df['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n    df['MONTHS_BALANCE_MAXX'] = gp['MONTHS_BALANCE'].max()\n    #percentage of previous loans completed and completed before initial term\n    df['POS_LOAN_COMPLTED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n    df['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n    df['POS_COMPLETED_BEFORE_MENN'] = df.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0\n                                              and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n    #number of remaining installments (future installments) and percentage from total\n    df['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALLMENT_FUTURE'].last()\n    df['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n    #group by SK_ID_CURR and merge\n    df_gp = df.groupby('SK_ID_CURR').sum().reset_index()\n    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace=True)\n    pos_agg = pd.merge(pos_agg, df_gp, on='SK_ID_CURR', how='left')\n    def df, gp, df_gp, sort_pos; gc.collect()\n    \n    #percentage of late payments for the 3 most recent applications\n    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n    #last month of each application\n    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n    #most recent applications (last 3)\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n    # drop some usless categories: too small. does not appear if you set num_rows=30000\n    drop_features = ['POS_NAME_CONTRACT_STATUS_Canceled_MEAN', 'POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN', 'POS_NAME_CONTRACT_STATUS_XNA_MEAN']\n    for drop_feature in drop_features:\n        try:\n            pos_agg.drop(drop_feature, axis=1, inplace=True)\n        except: pass\n    return pos_agg\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:02.919295Z","iopub.execute_input":"2022-10-10T10:16:02.919750Z","iopub.status.idle":"2022-10-10T10:16:02.961776Z","shell.execute_reply.started":"2022-10-10T10:16:02.919715Z","shell.execute_reply":"2022-10-10T10:16:02.960091Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"#def get_installment_payments(path, num_rows = None):\n#    \"\"\"Process installments_payments.csv and return a pandas dataframe\"\"\"\n#    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows=num_rows)\n#group payments and get payment difference\n    \n# below are copied\n\ndef get_installment_payments(path, num_rows= None):\n    \"\"\" Process installments_payments.csv and return a pandas dataframe. \"\"\"\n    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows= num_rows)\n    # Group payments and get Payment difference\n    pay = do_sum(pay, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n    pay['PAYMENT_DIFFERENCE'] = pay['AMT_INSTALMENT'] - pay['AMT_PAYMENT_GROUPED']\n    pay['PAYMENT_RATIO'] = pay['AMT_INSTALMENT'] / pay['AMT_PAYMENT_GROUPED']\n    pay['PAID_OVER_AMOUNT'] = pay['AMT_PAYMENT'] - pay['AMT_INSTALMENT']\n    pay['PAID_OVER'] = (pay['PAID_OVER_AMOUNT'] > 0).astype(int)\n    # Payment Entry: Days past due and Days before due\n    pay['DPD'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n    pay['DPD'] = pay['DPD'].apply(lambda x: 0 if x <= 0 else x)\n    pay['DBD'] = pay['DAYS_INSTALMENT'] - pay['DAYS_ENTRY_PAYMENT']\n    pay['DBD'] = pay['DBD'].apply(lambda x: 0 if x <= 0 else x)\n    # Flag late payment\n    pay['LATE_PAYMENT'] = pay['DBD'].apply(lambda x: 1 if x > 0 else 0)\n    # Percentage of payments that were late\n    pay['INSTALMENT_PAYMENT_RATIO'] = pay['AMT_PAYMENT'] / pay['AMT_INSTALMENT']\n    pay['LATE_PAYMENT_RATIO'] = pay.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n    # Flag late payments that have a significant amount\n    pay['SIGNIFICANT_LATE_PAYMENT'] = pay['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n    # Flag k threshold late payments\n    pay['DPD_7'] = pay['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n    pay['DPD_15'] = pay['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n    # Aggregations by SK_ID_CURR\n    pay_agg = group(pay, 'INS_', INSTALLMENTS_AGG)\n\n    # Installments in the last x months\n    for months in [36, 60]:\n        recent_prev_id = pay[pay['DAYS_INSTALMENT'] >= -30*months]['SK_ID_PREV'].unique()\n        pay_recent = pay[pay['SK_ID_PREV'].isin(recent_prev_id)]\n        prefix = 'INS_{}M_'.format(months)\n        pay_agg = group_and_merge(pay_recent, pay_agg, prefix, INSTALLMENTS_TIME_AGG)\n\n    # Last x periods trend features\n    group_features = ['SK_ID_CURR', 'SK_ID_PREV', 'DPD', 'LATE_PAYMENT',\n                      'PAID_OVER_AMOUNT', 'PAID_OVER', 'DAYS_INSTALMENT']\n    gp = pay[group_features].groupby('SK_ID_CURR')\n    func = partial(trend_in_last_k_instalment_features, periods= INSTALLMENTS_LAST_K_TREND_PERIODS)\n    g = parallel_apply(gp, func, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n\n    # Last loan features\n    g = parallel_apply(gp, installments_last_loan_features, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n    return pay_agg\n\n\ndef trend_in_last_k_instalment_features(gr, periods):\n    gr_ = gr.copy()\n    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n    features = {}\n\n    for period in periods:\n        gr_period = gr_.iloc[:period]\n        features = add_trend_feature(features, gr_period, 'DPD',\n                                           '{}_TREND_'.format(period))\n        features = add_trend_feature(features, gr_period, 'PAID_OVER_AMOUNT',\n                                           '{}_TREND_'.format(period))\n    return features\n\n\ndef installments_last_loan_features(gr):\n    gr_ = gr.copy()\n    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n\n    features = {}\n    features = add_features_in_group(features, gr_, 'DPD',\n                                     ['sum', 'mean', 'max', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'LATE_PAYMENT',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER_AMOUNT',\n                                     ['sum', 'mean', 'max', 'min', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    return features\n\n# ------------------------- CREDIT CARD PIPELINE -------------------------\n\ndef get_credit_card(path, num_rows= None):\n    \"\"\" Process credit_card_balance.csv and return a pandas dataframe. \"\"\"\n    cc = pd.read_csv(os.path.join(path, 'credit_card_balance.csv'), nrows= num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category=False)\n    cc.rename(columns={'AMT_RECIVABLE': 'AMT_RECEIVABLE'}, inplace=True)\n    # Amount used from limit\n    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n    # Current payment / Min payment\n    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n    # Late payment\n    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    # How much drawing of limit\n    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n    # Aggregations by SK_ID_CURR\n    cc_agg = cc.groupby('SK_ID_CURR').agg(CREDIT_CARD_AGG)\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    cc_agg.reset_index(inplace= True)\n\n    # Last month balance of each credit card application\n    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n    last_months_df = cc[cc.index.isin(last_ids)]\n    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n\n    # Aggregations for last x months\n    for months in [12, 24, 48]:\n        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n        prefix = 'INS_{}M_'.format(months)\n        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n    return cc_agg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# just copied codes\n\n\n# INSTALLMENTS TREND PERIODS\nINSTALLMENTS_LAST_K_TREND_PERIODS =  [12, 24, 60, 120]\n\n# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\nGENERATE_SUBMISSION_FILES = True\nSTRATIFIED_KFOLD = False\nRANDOM_SEED = 737851\nNUM_FOLDS = 10\nEARLY_STOPPING = 100\n\n\n\nLIGHTGBM_PARAMS = {\n    'boosting_type': 'goss',\n    'n_estimators': 10000,\n    'learning_rate': 0.005134,\n    'num_leaves': 54,\n    'max_depth': 10,\n    'subsample_for_bin': 240000,\n    'reg_alpha': 0.436193,\n    'reg_lambda': 0.479169,\n    'colsample_bytree': 0.508716,\n    'min_split_gain': 0.024766,\n    'subsample': 1,\n    'is_unbalance': False,\n    'silent':-1,\n    'verbose':-1\n}\n# AGGREGATIONS\nBUREAU_AGG = {\n    'SK_ID_BUREAU': ['nunique'],\n    'DAYS_CREDIT': ['min', 'max', 'mean'],\n    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n    'AMT_ANNUITY': ['mean'],\n    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n    # Categorical\n    'STATUS_0': ['mean'],\n    'STATUS_1': ['mean'],\n    'STATUS_12345': ['mean'],\n    'STATUS_C': ['mean'],\n    'STATUS_X': ['mean'],\n    'CREDIT_ACTIVE_Active': ['mean'],\n    'CREDIT_ACTIVE_Closed': ['mean'],\n    'CREDIT_ACTIVE_Sold': ['mean'],\n    'CREDIT_TYPE_Consumer credit': ['mean'],\n    'CREDIT_TYPE_Credit card': ['mean'],\n    'CREDIT_TYPE_Car loan': ['mean'],\n    'CREDIT_TYPE_Mortgage': ['mean'],\n    'CREDIT_TYPE_Microloan': ['mean'],\n    # Group by loan duration features (months)\n    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n    'LL_DEBT_CREDIT_DIFF': ['mean'],\n    'LL_STATUS_12345': ['mean'],\n}\n\nBUREAU_ACTIVE_AGG = {\n    'DAYS_CREDIT': ['max', 'mean'],\n    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM': ['max', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean'],\n    'DAYS_CREDIT_UPDATE': ['min', 'mean'],\n    'DEBT_PERCENTAGE': ['mean'],\n    'DEBT_CREDIT_DIFF': ['mean'],\n    'CREDIT_TO_ANNUITY_RATIO': ['mean'],\n    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n}\n\nBUREAU_CLOSED_AGG = {\n    'DAYS_CREDIT': ['max', 'var'],\n    'DAYS_CREDIT_ENDDATE': ['max'],\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n    'DAYS_CREDIT_UPDATE': ['max'],\n    'ENDDATE_DIF': ['mean'],\n    'STATUS_12345': ['mean'],\n}\n\nBUREAU_LOAN_TYPE_AGG = {\n    'DAYS_CREDIT': ['mean', 'max'],\n    'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n    'AMT_CREDIT_SUM': ['mean', 'max'],\n    'AMT_CREDIT_SUM_DEBT': ['mean', 'max'],\n    'DEBT_PERCENTAGE': ['mean'],\n    'DEBT_CREDIT_DIFF': ['mean'],\n    'DAYS_CREDIT_ENDDATE': ['max'],\n}\n\nBUREAU_TIME_AGG = {\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n    'AMT_CREDIT_SUM': ['max', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n    'DEBT_PERCENTAGE': ['mean'],\n    'DEBT_CREDIT_DIFF': ['mean'],\n    'STATUS_0': ['mean'],\n    'STATUS_12345': ['mean'],\n}\n\nPREVIOUS_AGG = {\n    'SK_ID_PREV': ['nunique'],\n    'AMT_ANNUITY': ['min', 'max', 'mean'],\n    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n    'CNT_PAYMENT': ['max', 'mean'],\n    'DAYS_TERMINATION': ['max'],\n    # Engineered features\n    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n}\n\nPREVIOUS_ACTIVE_AGG = {\n    'SK_ID_PREV': ['nunique'],\n    'SIMPLE_INTERESTS': ['mean'],\n    'AMT_ANNUITY': ['max', 'sum'],\n    'AMT_APPLICATION': ['max', 'mean'],\n    'AMT_CREDIT': ['sum'],\n    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n    'DAYS_DECISION': ['min', 'mean'],\n    'CNT_PAYMENT': ['mean', 'sum'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n    # Engineered features\n    'AMT_PAYMENT': ['sum'],\n    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n    'REPAYMENT_RATIO': ['mean'],\n}\n\nPREVIOUS_APPROVED_AGG = {\n    'SK_ID_PREV': ['nunique'],\n    'AMT_ANNUITY': ['min', 'max', 'mean'],\n    'AMT_CREDIT': ['min', 'max', 'mean'],\n    'AMT_DOWN_PAYMENT': ['max'],\n    'AMT_GOODS_PRICE': ['max'],\n    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n    'DAYS_DECISION': ['min', 'mean'],\n    'CNT_PAYMENT': ['max', 'mean'],\n    'DAYS_TERMINATION': ['mean'],\n    # Engineered features\n    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n    'APPLICATION_CREDIT_DIFF': ['max'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n    # The following features are only for approved applications\n    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n    'DAYS_FIRST_DUE': ['min', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n    'DAYS_LAST_DUE': ['max', 'mean'],\n    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n}\n\nPREVIOUS_REFUSED_AGG = {\n    'AMT_APPLICATION': ['max', 'mean'],\n    'AMT_CREDIT': ['min', 'max'],\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n    'CNT_PAYMENT': ['max', 'mean'],\n    # Engineered features\n    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n}\n\nPREVIOUS_LATE_PAYMENTS_AGG = {\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n    # Engineered features\n    'APPLICATION_CREDIT_DIFF': ['min'],\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n}\n\nPREVIOUS_LOAN_TYPE_AGG = {\n    'AMT_CREDIT': ['sum'],\n    'AMT_ANNUITY': ['mean', 'max'],\n    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n    'DAYS_DECISION': ['max'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n    'CNT_PAYMENT': ['mean'],\n}\n\nPREVIOUS_TIME_AGG = {\n    'AMT_CREDIT': ['sum'],\n    'AMT_ANNUITY': ['mean', 'max'],\n    'SIMPLE_INTERESTS': ['mean', 'max'],\n    'DAYS_DECISION': ['min', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n    # Engineered features\n    'APPLICATION_CREDIT_DIFF': ['min'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n}\n\nPOS_CASH_AGG = {\n    'SK_ID_PREV': ['nunique'],\n    'MONTHS_BALANCE': ['min', 'max', 'size'],\n    'SK_DPD': ['max', 'mean', 'sum', 'var'],\n    'SK_DPD_DEF': ['max', 'mean', 'sum'],\n    'LATE_PAYMENT': ['mean']\n}\n\nINSTALLMENTS_AGG = {\n    'SK_ID_PREV': ['size', 'nunique'],\n    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n    'DPD': ['max', 'mean', 'var'],\n    'DBD': ['max', 'mean', 'var'],\n    'PAYMENT_DIFFERENCE': ['mean'],\n    'PAYMENT_RATIO': ['mean'],\n    'LATE_PAYMENT': ['mean', 'sum'],\n    'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n    'LATE_PAYMENT_RATIO': ['mean'],\n    'DPD_7': ['mean'],\n    'DPD_15': ['mean'],\n    'PAID_OVER': ['mean']\n}\n\nINSTALLMENTS_TIME_AGG = {\n    'SK_ID_PREV': ['size'],\n    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n    'DPD': ['max', 'mean', 'var'],\n    'DBD': ['max', 'mean', 'var'],\n    'PAYMENT_DIFFERENCE': ['mean'],\n    'PAYMENT_RATIO': ['mean'],\n    'LATE_PAYMENT': ['mean'],\n    'SIGNIFICANT_LATE_PAYMENT': ['mean'],\n    'LATE_PAYMENT_RATIO': ['mean'],\n    'DPD_7': ['mean'],\n    'DPD_15': ['mean'],\n}\n\nCREDIT_CARD_AGG = {\n    'MONTHS_BALANCE': ['min'],\n    'AMT_BALANCE': ['max'],\n    'AMT_CREDIT_LIMIT_ACTUAL': ['max'],\n    'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n    'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n    'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n    'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n    'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean', 'sum', 'var'],\n    'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n    'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean', 'sum'],\n    'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n    'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n    'SK_DPD': ['mean', 'max', 'sum'],\n    'SK_DPD_DEF': ['max', 'sum'],\n    'LIMIT_USE': ['max', 'mean'],\n    'PAYMENT_DIV_MIN': ['min', 'mean'],\n    'LATE_PAYMENT': ['max', 'sum'],\n}\n\nCREDIT_CARD_TIME_AGG = {\n    'CNT_DRAWINGS_ATM_CURRENT': ['mean'],\n    'SK_DPD': ['max', 'sum'],\n    'AMT_BALANCE': ['mean', 'max'],\n    'LIMIT_USE': ['max', 'mean']\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:33.612001Z","iopub.execute_input":"2022-10-10T10:16:33.612371Z","iopub.status.idle":"2022-10-10T10:16:33.650711Z","shell.execute_reply.started":"2022-10-10T10:16:33.612341Z","shell.execute_reply":"2022-10-10T10:16:33.649347Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Sequantial execution\n풀어 씁니다.","metadata":{}},{"cell_type":"code","source":"warnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:36.540853Z","iopub.execute_input":"2022-10-10T10:16:36.541421Z","iopub.status.idle":"2022-10-10T10:16:36.546669Z","shell.execute_reply.started":"2022-10-10T10:16:36.541375Z","shell.execute_reply":"2022-10-10T10:16:36.545724Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 60)\npd.set_option('display.max_columns', 100)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:36.883212Z","iopub.execute_input":"2022-10-10T10:16:36.884362Z","iopub.status.idle":"2022-10-10T10:16:36.888607Z","shell.execute_reply.started":"2022-10-10T10:16:36.884321Z","shell.execute_reply":"2022-10-10T10:16:36.887658Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"num_rows = 30000\ndebug = True\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:38.443463Z","iopub.execute_input":"2022-10-10T10:16:38.444917Z","iopub.status.idle":"2022-10-10T10:16:38.449800Z","shell.execute_reply.started":"2022-10-10T10:16:38.444871Z","shell.execute_reply":"2022-10-10T10:16:38.448681Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"NUM_THREADS=4\nDATA_DIRECTORY=\"../input/home-credit-default-risk/\"\nSUBMISSION_SUFIX=\"_model2_04\"\npath=DATA_DIRECTORY\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:41.213293Z","iopub.execute_input":"2022-10-10T10:16:41.213691Z","iopub.status.idle":"2022-10-10T10:16:41.218413Z","shell.execute_reply.started":"2022-10-10T10:16:41.213658Z","shell.execute_reply":"2022-10-10T10:16:41.217670Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"with timer(\"application_train and application_test\"):\n    df = get_train_test(DATA_DIRECTORY, num_rows=num_rows)\n    print(\"Application dataframe shape: \", df.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:16:43.183710Z","iopub.execute_input":"2022-10-10T10:16:43.184147Z","iopub.status.idle":"2022-10-10T10:16:52.206872Z","shell.execute_reply.started":"2022-10-10T10:16:43.184111Z","shell.execute_reply":"2022-10-10T10:16:52.205641Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Application dataframe shape:  (59999, 83)\napplication_train and application_test - done in 9s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"__do_mean 등 utility 함수 훑어보기__\n간단한 데이터프레임을 만들어 테스트 해봅시다. 이거는 group cols\n```python\ngp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n    columns={counted: agg_name})\ndf = df.merge(gp, on=group_cols, how='left')\n```","metadata":{}},{"cell_type":"code","source":"with timer(\"Bureau and bureau_balancce data\"):\n    bureau_df=get_bureau(DATA_DIRECTORY, num_rows=num_rows)\n    df = pd.merge(df, bureau_df, on='SK_ID_CURR', how='left')\n    print(\"Bureau dataframe shape: \", bureau_df.shape)\n    del bureau_df; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:17:08.544265Z","iopub.execute_input":"2022-10-10T10:17:08.545201Z","iopub.status.idle":"2022-10-10T10:17:10.617864Z","shell.execute_reply.started":"2022-10-10T10:17:08.545160Z","shell.execute_reply":"2022-10-10T10:17:10.616545Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Bureau dataframe shape:  (6076, 156)\nBureau and bureau_balancce data - done in 2s\n","output_type":"stream"}]},{"cell_type":"code","source":"with timer(\"previous_application\"):\n    prev_df = get_previous_applications(DATA_DIRECTORY, num_rows)\n    df=pd.merge(df, prev_df, on='SK_ID_CURR', how='left')\n    del prev_df; gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:17:10.619382Z","iopub.execute_input":"2022-10-10T10:17:10.619738Z","iopub.status.idle":"2022-10-10T10:17:13.198502Z","shell.execute_reply.started":"2022-10-10T10:17:10.619708Z","shell.execute_reply":"2022-10-10T10:17:13.197351Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"},{"name":"stdout","text":"previous_application - done in 3s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## GET POS CASH 함수 분석: sequential execution에서는 건너뛰어야 합니다.\n```python\npos=get_pos_cash(DATA_DIRECTORY, num_rows)\ndf=pd.merge(df, pos, on='SK_ID_CURR', how='left')\n```\n이 부분부터 나눠서 뜯어봅시다.","metadata":{}},{"cell_type":"code","source":"pos = pd.read_csv(os.path.join(path, 'POS_CASH_balance.csv'), nrows=num_rows)\npos.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:18.955277Z","iopub.execute_input":"2022-10-10T08:58:18.955569Z","iopub.status.idle":"2022-10-10T08:58:18.988513Z","shell.execute_reply.started":"2022-10-10T08:58:18.955544Z","shell.execute_reply":"2022-10-10T08:58:18.986914Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   SK_ID_PREV  SK_ID_CURR  MONTHS_BALANCE  CNT_INSTALMENT  \\\n0     1803195      182943             -31            48.0   \n1     1715348      367990             -33            36.0   \n2     1784872      397406             -32            12.0   \n3     1903291      269225             -35            48.0   \n4     2341044      334279             -35            36.0   \n\n   CNT_INSTALMENT_FUTURE NAME_CONTRACT_STATUS  SK_DPD  SK_DPD_DEF  \n0                   45.0               Active       0           0  \n1                   35.0               Active       0           0  \n2                    9.0               Active       0           0  \n3                   42.0               Active       0           0  \n4                   35.0               Active       0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_PREV</th>\n      <th>SK_ID_CURR</th>\n      <th>MONTHS_BALANCE</th>\n      <th>CNT_INSTALMENT</th>\n      <th>CNT_INSTALMENT_FUTURE</th>\n      <th>NAME_CONTRACT_STATUS</th>\n      <th>SK_DPD</th>\n      <th>SK_DPD_DEF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1803195</td>\n      <td>182943</td>\n      <td>-31</td>\n      <td>48.0</td>\n      <td>45.0</td>\n      <td>Active</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1715348</td>\n      <td>367990</td>\n      <td>-33</td>\n      <td>36.0</td>\n      <td>35.0</td>\n      <td>Active</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1784872</td>\n      <td>397406</td>\n      <td>-32</td>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>Active</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1903291</td>\n      <td>269225</td>\n      <td>-35</td>\n      <td>48.0</td>\n      <td>42.0</td>\n      <td>Active</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2341044</td>\n      <td>334279</td>\n      <td>-35</td>\n      <td>36.0</td>\n      <td>35.0</td>\n      <td>Active</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"데이터 설명에서 이 파일에 대한 부분입니다만 ID부분을 제외하고 깊게 알아볼 필요는 없을 것 같습니다.\n- SK_ID_PREV : 과거 Home Credit에서의 융자(credit) ID. 샘플에서 한 대출(loan)은 0~2개의 과거 융자(loan)와 연결될 수 있음 - loan과 credit을 크게 구별하지 않는 것 같습니다.\n- SK_ID_CURR : 샘플에서 대출(loan)의 ID\n- MONTHS_BALANCE : 대출 신청일과 balance(잔고?잔액?) 데이터가 몇개월이나 차이나는지. -1은 매달 확인하는 데이터의 자료라는 뜻이고, 0은 대출을 신청한 시점에서의 데이터라는 뜻입니다. 그런데 은행들이 Credit Bureau측에 업데이트를 자주 안해줘서 0이나 -1이나 의미 없을 수 있다고 하네요.\n- CNT_INSTALMENT: Term of previous credit (can change over time)\n- CNT_INSTALMENT_FUTURE: Installments left to pay on the previous credit\n- NAME_CONTRACT_STATUS: Contract status during the month\n- SK_DPD: 지난 융자에서 기한을 얼마나 넘겼는지.\n- SK_DPD_DEF: PD during the month with tolerance (debts with low loan amounts are ignored) of the previous credit","metadata":{}},{"cell_type":"code","source":"#print(pos['SK_ID_PREV'].nunique(), pos['SK_ID_PREV'].value_counts())\nprint(pos['SK_ID_PREV'].nunique())\n#print(pos.groupby('SK_ID_PREV')['SK_ID_CURR'].nunique().sum())\nprint(pos['SK_ID_CURR'].nunique())\n#print(pos.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique().sum())","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:18.990527Z","iopub.execute_input":"2022-10-10T08:58:18.991304Z","iopub.status.idle":"2022-10-10T08:58:19.001564Z","shell.execute_reply.started":"2022-10-10T08:58:18.991264Z","shell.execute_reply":"2022-10-10T08:58:19.000643Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"28559\n27135\n","output_type":"stream"}]},{"cell_type":"code","source":"pos.groupby('SK_ID_CURR')[['SK_ID_CURR', 'SK_ID_PREV']].nunique().sort_values(by='SK_ID_PREV').max()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:19.002644Z","iopub.execute_input":"2022-10-10T08:58:19.003679Z","iopub.status.idle":"2022-10-10T08:58:19.033610Z","shell.execute_reply.started":"2022-10-10T08:58:19.003650Z","shell.execute_reply":"2022-10-10T08:58:19.031827Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"SK_ID_CURR    1\nSK_ID_PREV    4\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"pos.groupby('SK_ID_PREV')[['SK_ID_CURR', 'SK_ID_PREV']].nunique().max()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:19.035038Z","iopub.execute_input":"2022-10-10T08:58:19.035376Z","iopub.status.idle":"2022-10-10T08:58:19.062948Z","shell.execute_reply.started":"2022-10-10T08:58:19.035347Z","shell.execute_reply":"2022-10-10T08:58:19.061675Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"SK_ID_CURR    1\nSK_ID_PREV    1\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"SK_ID_PREV, SK_ID_CURR 의 경우 description에서 설명한 대로 중복값이 꽤 있습니다. 그런데 어떤 식으로 중복값이 있을지 확인해 보았습니다. PREV->CURR 는 다대일 대응관계가 성립합니다.","metadata":{}},{"cell_type":"code","source":"#pos[pos['SK_ID_PREV'].duplicated(keep=False)].sort_values(by='SK_ID_PREV', axis=0).head()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:19.066377Z","iopub.execute_input":"2022-10-10T08:58:19.066762Z","iopub.status.idle":"2022-10-10T08:58:19.072747Z","shell.execute_reply.started":"2022-10-10T08:58:19.066732Z","shell.execute_reply":"2022-10-10T08:58:19.071490Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"pos, categorical_cols = one_hot_encoder(pos, nan_as_category= False)\ncategorical_cols","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:19.074363Z","iopub.execute_input":"2022-10-10T08:58:19.074669Z","iopub.status.idle":"2022-10-10T08:58:19.094893Z","shell.execute_reply.started":"2022-10-10T08:58:19.074637Z","shell.execute_reply":"2022-10-10T08:58:19.092685Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['NAME_CONTRACT_STATUS_Active',\n 'NAME_CONTRACT_STATUS_Approved',\n 'NAME_CONTRACT_STATUS_Completed',\n 'NAME_CONTRACT_STATUS_Demand',\n 'NAME_CONTRACT_STATUS_Returned to the store',\n 'NAME_CONTRACT_STATUS_Signed']"},"metadata":{}}]},{"cell_type":"markdown","source":"만약 nrows를 제한하지 않는다면 NAME_CONTRACT_STATUS의 분포는 다음과 같습니다.  \n```python\npos = pd.read_csv(os.path.join(path, 'POS_CASH_balance.csv'), nrows=None)\npos['NAME_CONTRACT_STATUS'].value_counts()\n```\n```python\nActive                   9151119  \nCompleted                 744883  \nSigned                     87260  \nDemand                      7065  \nReturned to the store       5461  \nApproved                    4917  \nAmortized debt               636  \nCanceled                      15  \nXNA                            2  \nName: NAME_CONTRACT_STATUS, dtype: int64  \n```","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:54:01.741174Z","iopub.execute_input":"2022-10-10T08:54:01.741609Z","iopub.status.idle":"2022-10-10T08:54:02.268347Z","shell.execute_reply.started":"2022-10-10T08:54:01.741557Z","shell.execute_reply":"2022-10-10T08:54:02.266782Z"}}},{"cell_type":"code","source":"#pos['NAME_CONTRACT_STATUS'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T08:58:19.117305Z","iopub.execute_input":"2022-10-10T08:58:19.118657Z","iopub.status.idle":"2022-10-10T08:58:19.133789Z","shell.execute_reply.started":"2022-10-10T08:58:19.118579Z","shell.execute_reply":"2022-10-10T08:58:19.131729Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Back to sequential execution","metadata":{}},{"cell_type":"code","source":"with timer(\"previous applications balance\"):\n    pos=get_pos_cash(DATA_DIRECTORY, num_rows)\n    df=pd.merge(df, pos, on='SK_ID_CURR', how='left')\n    print(\"Pos-cash dataframe shape: \", pos.shape)\n    del pos; gc.collect()\n    ins = get_installment_payments(DATA_DIRECTORY, num_rows)\n    df=pd.merge(df, ins, on='SK_ID_CURR', how='left')\n    print(\"Installments dataframe shape: \", ins.shape)\n    del ins; gc.collect()\n    cc=get_credit_card(DATA_DIRECTORY, num_rows)\n    df=pd.merge(df, cc, on='SK_ID_CURR', how='left')\n    print(\"Credit card dataframe shape: \", cc.shape)\n    del cc; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:17:24.854845Z","iopub.execute_input":"2022-10-10T10:17:24.855230Z","iopub.status.idle":"2022-10-10T10:18:30.204154Z","shell.execute_reply.started":"2022-10-10T10:17:24.855198Z","shell.execute_reply":"2022-10-10T10:18:30.202770Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Pos-cash dataframe shape:  (27135, 24)\nInstallments dataframe shape:  (22446, 101)\nCredit card dataframe shape:  (24459, 59)\nprevious applications balance - done in 65s\n","output_type":"stream"}]},{"cell_type":"code","source":"#print([c for c in df.columns if 'BUREAU' in c])","metadata":{"execution":{"iopub.status.busy":"2022-10-10T09:53:32.849209Z","iopub.execute_input":"2022-10-10T09:53:32.849568Z","iopub.status.idle":"2022-10-10T09:53:32.854773Z","shell.execute_reply.started":"2022-10-10T09:53:32.849532Z","shell.execute_reply":"2022-10-10T09:53:32.853521Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#add ratios and groupby between different tables\ndf=add_ratios_features(df)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:18:30.206369Z","iopub.execute_input":"2022-10-10T10:18:30.206823Z","iopub.status.idle":"2022-10-10T10:18:30.230762Z","shell.execute_reply.started":"2022-10-10T10:18:30.206774Z","shell.execute_reply":"2022-10-10T10:18:30.229703Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"\ndf=reduce_memory(df)\nlgbm_categorical_feat=[\n    'CODE_GENDER', 'FLAG_OWN_CAR','NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE',\n    'ORGANIZATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_TYPE_SUITE', 'WALLSMATERIAL_MODE']\nwith timer(\"Run LightGBM\"):\n    feat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n    print(feat_importance)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:30:52.844937Z","iopub.execute_input":"2022-10-10T10:30:52.845848Z","iopub.status.idle":"2022-10-10T10:36:21.766699Z","shell.execute_reply.started":"2022-10-10T10:30:52.845811Z","shell.execute_reply":"2022-10-10T10:36:21.765457Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Initial df memory usage is 109.12 MB for 659 columns\nFinal memory usage is: 109.12 MB - decreased by 0.0%\nTrain/valid shape: (29999, 659), test_shape: (30000, 659)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.879428\ttraining's binary_logloss: 0.211405\tvalid_1's auc: 0.737034\tvalid_1's binary_logloss: 0.238554\n[800]\ttraining's auc: 0.933191\ttraining's binary_logloss: 0.181833\tvalid_1's auc: 0.74501\tvalid_1's binary_logloss: 0.235783\nFOLD  1 AUC : 0.746129\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"FOLD  2 AUC : 0.733222\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.880774\ttraining's binary_logloss: 0.211802\tvalid_1's auc: 0.749014\tvalid_1's binary_logloss: 0.232161\n[800]\ttraining's auc: 0.934099\ttraining's binary_logloss: 0.182049\tvalid_1's auc: 0.75287\tvalid_1's binary_logloss: 0.229843\n[1200]\ttraining's auc: 0.963374\ttraining's binary_logloss: 0.161232\tvalid_1's auc: 0.755506\tvalid_1's binary_logloss: 0.228716\nFOLD  3 AUC : 0.755719\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"FOLD  4 AUC : 0.723667\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.882525\ttraining's binary_logloss: 0.208909\tvalid_1's auc: 0.718882\tvalid_1's binary_logloss: 0.255484\nFOLD  5 AUC : 0.722983\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.882934\ttraining's binary_logloss: 0.209785\tvalid_1's auc: 0.741746\tvalid_1's binary_logloss: 0.249022\nFOLD  6 AUC : 0.742152\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"FOLD  7 AUC : 0.734142\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.880985\ttraining's binary_logloss: 0.212175\tvalid_1's auc: 0.717321\tvalid_1's binary_logloss: 0.229118\n[800]\ttraining's auc: 0.933432\ttraining's binary_logloss: 0.182615\tvalid_1's auc: 0.728971\tvalid_1's binary_logloss: 0.226331\n[1200]\ttraining's auc: 0.963267\ttraining's binary_logloss: 0.161198\tvalid_1's auc: 0.733498\tvalid_1's binary_logloss: 0.225126\nFOLD  8 AUC : 0.735158\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[400]\ttraining's auc: 0.882288\ttraining's binary_logloss: 0.207467\tvalid_1's auc: 0.759756\tvalid_1's binary_logloss: 0.269797\n[800]\ttraining's auc: 0.935686\ttraining's binary_logloss: 0.177928\tvalid_1's auc: 0.766963\tvalid_1's binary_logloss: 0.266818\nFOLD  9 AUC : 0.767694\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\nNew categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n  _log_warning('categorical_feature in Dataset is overridden.\\n'\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"FOLD 10 AUC : 0.777957\nFull AUC score 0.724187\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"},{"name":"stdout","text":"                                    feature          gain  split\n280                        EXT_SOURCES_MEAN  50420.774443  766.7\n282                   EXT_SOURCES_NANMEDIAN  30954.682970  662.5\n288                            EXT_SOURCE_3  16707.850373  971.7\n281                         EXT_SOURCES_MIN  15867.782935  709.9\n254                 CREDIT_TO_ANNUITY_RATIO  12062.662833  988.3\n..                                      ...           ...    ...\n94   BUREAU_CAR_AMT_CREDIT_MAX_OVERDUE_MEAN      0.000000    0.0\n242                     CC_LATE_PAYMENT_SUM      0.000000    0.0\n241                     CC_LATE_PAYMENT_MAX      0.000000    0.0\n95       BUREAU_CAR_AMT_CREDIT_SUM_DEBT_MAX      0.000000    0.0\n98           BUREAU_CAR_AMT_CREDIT_SUM_MEAN      0.000000    0.0\n\n[657 rows x 3 columns]\nRun LightGBM - done in 327s\n","output_type":"stream"}]},{"cell_type":"code","source":"[c for c in df.columns if \"PREV\" in c]","metadata":{"execution":{"iopub.status.busy":"2022-10-10T10:26:44.544626Z","iopub.execute_input":"2022-10-10T10:26:44.545713Z","iopub.status.idle":"2022-10-10T10:26:44.557201Z","shell.execute_reply.started":"2022-10-10T10:26:44.545663Z","shell.execute_reply":"2022-10-10T10:26:44.556027Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"['PREV_SK_ID_PREV_NUNIQUE',\n 'PREV_AMT_ANNUITY_MIN',\n 'PREV_AMT_ANNUITY_MAX',\n 'PREV_AMT_ANNUITY_MEAN',\n 'PREV_AMT_DOWN_PAYMENT_MAX',\n 'PREV_AMT_DOWN_PAYMENT_MEAN',\n 'PREV_HOUR_APPR_PROCESS_START_MIN',\n 'PREV_HOUR_APPR_PROCESS_START_MAX',\n 'PREV_HOUR_APPR_PROCESS_START_MEAN',\n 'PREV_RATE_DOWN_PAYMENT_MAX',\n 'PREV_RATE_DOWN_PAYMENT_MEAN',\n 'PREV_DAYS_DECISION_MIN',\n 'PREV_DAYS_DECISION_MAX',\n 'PREV_DAYS_DECISION_MEAN',\n 'PREV_CNT_PAYMENT_MAX',\n 'PREV_CNT_PAYMENT_MEAN',\n 'PREV_DAYS_TERMINATION_MAX',\n 'PREV_CREDIT_TO_ANNUITY_RATIO_MEAN',\n 'PREV_CREDIT_TO_ANNUITY_RATIO_MAX',\n 'PREV_APPLICATION_CREDIT_DIFF_MIN',\n 'PREV_APPLICATION_CREDIT_DIFF_MAX',\n 'PREV_APPLICATION_CREDIT_DIFF_MEAN',\n 'PREV_APPLICATION_CREDIT_RATIO_MIN',\n 'PREV_APPLICATION_CREDIT_RATIO_MAX',\n 'PREV_APPLICATION_CREDIT_RATIO_MEAN',\n 'PREV_APPLICATION_CREDIT_RATIO_VAR',\n 'PREV_DOWN_PAYMENT_TO_CREDIT_MEAN',\n 'PREV_NAME_CONTRACT_STATUS_Approved_MEAN',\n 'PREV_NAME_CONTRACT_STATUS_Canceled_MEAN',\n 'PREV_NAME_CONTRACT_STATUS_Refused_MEAN',\n 'PREV_NAME_CONTRACT_STATUS_Unused offer_MEAN',\n 'PREV_NAME_CONTRACT_TYPE_Cash loans_MEAN',\n 'PREV_NAME_CONTRACT_TYPE_Consumer loans_MEAN',\n 'PREV_NAME_CONTRACT_TYPE_Revolving loans_MEAN',\n 'PREV_NAME_CONTRACT_TYPE_XNA_MEAN',\n 'PREV_CHANNEL_TYPE_AP+ (Cash loan)_MEAN',\n 'PREV_CHANNEL_TYPE_Car dealer_MEAN',\n 'PREV_CHANNEL_TYPE_Channel of corporate sales_MEAN',\n 'PREV_CHANNEL_TYPE_Contact center_MEAN',\n 'PREV_CHANNEL_TYPE_Country-wide_MEAN',\n 'PREV_CHANNEL_TYPE_Credit and cash offices_MEAN',\n 'PREV_CHANNEL_TYPE_Regional / Local_MEAN',\n 'PREV_CHANNEL_TYPE_Stone_MEAN',\n 'PREV_NAME_TYPE_SUITE_Children_MEAN',\n 'PREV_NAME_TYPE_SUITE_Family_MEAN',\n 'PREV_NAME_TYPE_SUITE_Group of people_MEAN',\n 'PREV_NAME_TYPE_SUITE_Other_A_MEAN',\n 'PREV_NAME_TYPE_SUITE_Other_B_MEAN',\n 'PREV_NAME_TYPE_SUITE_Spouse, partner_MEAN',\n 'PREV_NAME_TYPE_SUITE_Unaccompanied_MEAN',\n 'PREV_NAME_YIELD_GROUP_XNA_MEAN',\n 'PREV_NAME_YIELD_GROUP_high_MEAN',\n 'PREV_NAME_YIELD_GROUP_low_action_MEAN',\n 'PREV_NAME_YIELD_GROUP_low_normal_MEAN',\n 'PREV_NAME_YIELD_GROUP_middle_MEAN',\n 'PREV_PRODUCT_COMBINATION_Card Street_MEAN',\n 'PREV_PRODUCT_COMBINATION_Card X-Sell_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash Street: high_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash Street: low_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash Street: middle_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash X-Sell: high_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash X-Sell: low_MEAN',\n 'PREV_PRODUCT_COMBINATION_Cash X-Sell: middle_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS household with interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS household without interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS industry with interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS industry without interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS mobile with interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS mobile without interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS other with interest_MEAN',\n 'PREV_PRODUCT_COMBINATION_POS others without interest_MEAN',\n 'PREV_NAME_PRODUCT_TYPE_XNA_MEAN',\n 'PREV_NAME_PRODUCT_TYPE_walk-in_MEAN',\n 'PREV_NAME_PRODUCT_TYPE_x-sell_MEAN',\n 'PREV_NAME_CLIENT_TYPE_New_MEAN',\n 'PREV_NAME_CLIENT_TYPE_Refreshed_MEAN',\n 'PREV_NAME_CLIENT_TYPE_Repeater_MEAN',\n 'PREV_NAME_CLIENT_TYPE_XNA_MEAN',\n 'PREV_ACTIVE_SK_ID_PREV_NUNIQUE',\n 'PREV_ACTIVE_SIMPLE_INTERESTS_MEAN',\n 'PREV_ACTIVE_AMT_ANNUITY_MAX',\n 'PREV_ACTIVE_AMT_ANNUITY_SUM',\n 'PREV_ACTIVE_AMT_APPLICATION_MAX',\n 'PREV_ACTIVE_AMT_APPLICATION_MEAN',\n 'PREV_ACTIVE_AMT_CREDIT_SUM',\n 'PREV_ACTIVE_AMT_DOWN_PAYMENT_MAX',\n 'PREV_ACTIVE_AMT_DOWN_PAYMENT_MEAN',\n 'PREV_ACTIVE_DAYS_DECISION_MIN',\n 'PREV_ACTIVE_DAYS_DECISION_MEAN',\n 'PREV_ACTIVE_CNT_PAYMENT_MEAN',\n 'PREV_ACTIVE_CNT_PAYMENT_SUM',\n 'PREV_ACTIVE_DAYS_LAST_DUE_1ST_VERSION_MIN',\n 'PREV_ACTIVE_DAYS_LAST_DUE_1ST_VERSION_MAX',\n 'PREV_ACTIVE_DAYS_LAST_DUE_1ST_VERSION_MEAN',\n 'PREV_ACTIVE_AMT_PAYMENT_SUM',\n 'PREV_ACTIVE_INSTALMENT_PAYMENT_DIFF_MEAN',\n 'PREV_ACTIVE_INSTALMENT_PAYMENT_DIFF_MAX',\n 'PREV_ACTIVE_REMAINING_DEBT_MAX',\n 'PREV_ACTIVE_REMAINING_DEBT_MEAN',\n 'PREV_ACTIVE_REMAINING_DEBT_SUM',\n 'PREV_ACTIVE_REPAYMENT_RATIO_MEAN',\n 'APPROVED_SK_ID_PREV_NUNIQUE',\n 'PREV_Consumer_AMT_CREDIT_SUM',\n 'PREV_Consumer_AMT_ANNUITY_MEAN',\n 'PREV_Consumer_AMT_ANNUITY_MAX',\n 'PREV_Consumer_SIMPLE_INTERESTS_MIN',\n 'PREV_Consumer_SIMPLE_INTERESTS_MEAN',\n 'PREV_Consumer_SIMPLE_INTERESTS_MAX',\n 'PREV_Consumer_SIMPLE_INTERESTS_VAR',\n 'PREV_Consumer_APPLICATION_CREDIT_DIFF_MIN',\n 'PREV_Consumer_APPLICATION_CREDIT_DIFF_VAR',\n 'PREV_Consumer_APPLICATION_CREDIT_RATIO_MIN',\n 'PREV_Consumer_APPLICATION_CREDIT_RATIO_MAX',\n 'PREV_Consumer_APPLICATION_CREDIT_RATIO_MEAN',\n 'PREV_Consumer_DAYS_DECISION_MAX',\n 'PREV_Consumer_DAYS_LAST_DUE_1ST_VERSION_MAX',\n 'PREV_Consumer_DAYS_LAST_DUE_1ST_VERSION_MEAN',\n 'PREV_Consumer_CNT_PAYMENT_MEAN',\n 'PREV_Cash_AMT_CREDIT_SUM',\n 'PREV_Cash_AMT_ANNUITY_MEAN',\n 'PREV_Cash_AMT_ANNUITY_MAX',\n 'PREV_Cash_SIMPLE_INTERESTS_MIN',\n 'PREV_Cash_SIMPLE_INTERESTS_MEAN',\n 'PREV_Cash_SIMPLE_INTERESTS_MAX',\n 'PREV_Cash_SIMPLE_INTERESTS_VAR',\n 'PREV_Cash_APPLICATION_CREDIT_DIFF_MIN',\n 'PREV_Cash_APPLICATION_CREDIT_DIFF_VAR',\n 'PREV_Cash_APPLICATION_CREDIT_RATIO_MIN',\n 'PREV_Cash_APPLICATION_CREDIT_RATIO_MAX',\n 'PREV_Cash_APPLICATION_CREDIT_RATIO_MEAN',\n 'PREV_Cash_DAYS_DECISION_MAX',\n 'PREV_Cash_DAYS_LAST_DUE_1ST_VERSION_MAX',\n 'PREV_Cash_DAYS_LAST_DUE_1ST_VERSION_MEAN',\n 'PREV_Cash_CNT_PAYMENT_MEAN',\n 'PREV_LAST12M_AMT_CREDIT_SUM',\n 'PREV_LAST12M_AMT_ANNUITY_MEAN',\n 'PREV_LAST12M_AMT_ANNUITY_MAX',\n 'PREV_LAST12M_SIMPLE_INTERESTS_MEAN',\n 'PREV_LAST12M_SIMPLE_INTERESTS_MAX',\n 'PREV_LAST12M_DAYS_DECISION_MIN',\n 'PREV_LAST12M_DAYS_DECISION_MEAN',\n 'PREV_LAST12M_DAYS_LAST_DUE_1ST_VERSION_MIN',\n 'PREV_LAST12M_DAYS_LAST_DUE_1ST_VERSION_MAX',\n 'PREV_LAST12M_DAYS_LAST_DUE_1ST_VERSION_MEAN',\n 'PREV_LAST12M_APPLICATION_CREDIT_DIFF_MIN',\n 'PREV_LAST12M_APPLICATION_CREDIT_RATIO_MIN',\n 'PREV_LAST12M_APPLICATION_CREDIT_RATIO_MAX',\n 'PREV_LAST12M_APPLICATION_CREDIT_RATIO_MEAN',\n 'PREV_LAST12M_NAME_CONTRACT_TYPE_Consumer loans_MEAN',\n 'PREV_LAST12M_NAME_CONTRACT_TYPE_Cash loans_MEAN',\n 'PREV_LAST12M_NAME_CONTRACT_TYPE_Revolving loans_MEAN',\n 'PREV_LAST24M_AMT_CREDIT_SUM',\n 'PREV_LAST24M_AMT_ANNUITY_MEAN',\n 'PREV_LAST24M_AMT_ANNUITY_MAX',\n 'PREV_LAST24M_SIMPLE_INTERESTS_MEAN',\n 'PREV_LAST24M_SIMPLE_INTERESTS_MAX',\n 'PREV_LAST24M_DAYS_DECISION_MIN',\n 'PREV_LAST24M_DAYS_DECISION_MEAN',\n 'PREV_LAST24M_DAYS_LAST_DUE_1ST_VERSION_MIN',\n 'PREV_LAST24M_DAYS_LAST_DUE_1ST_VERSION_MAX',\n 'PREV_LAST24M_DAYS_LAST_DUE_1ST_VERSION_MEAN',\n 'PREV_LAST24M_APPLICATION_CREDIT_DIFF_MIN',\n 'PREV_LAST24M_APPLICATION_CREDIT_RATIO_MIN',\n 'PREV_LAST24M_APPLICATION_CREDIT_RATIO_MAX',\n 'PREV_LAST24M_APPLICATION_CREDIT_RATIO_MEAN',\n 'PREV_LAST24M_NAME_CONTRACT_TYPE_Consumer loans_MEAN',\n 'PREV_LAST24M_NAME_CONTRACT_TYPE_Cash loans_MEAN',\n 'PREV_LAST24M_NAME_CONTRACT_TYPE_Revolving loans_MEAN',\n 'POS_SK_ID_PREV_NUNIQUE',\n 'INS_SK_ID_PREV_SIZE',\n 'INS_SK_ID_PREV_NUNIQUE',\n 'INS_36M_SK_ID_PREV_SIZE',\n 'INS_60M_SK_ID_PREV_SIZE']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}