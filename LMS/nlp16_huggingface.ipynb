{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc6f8a5",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "this notebook is written by changhyun yi, taking Modulab's Aiffel Program (Deep learning education course).  \n",
    "이 노트북은 모두의연구소 AIFFEL 과정 중 going-deeper node 16, hugging face custom model 만들기에 제출하기 위해 만들었습니다.\n",
    "\n",
    "# mnli task\n",
    "MNLI task에 대한 description은 [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) 에서 찾을 수 있습니다. 두 문장이 __전제(premise)__ 와 __가설(hypothesis)__ 로 제시되고 그 사이의 관계를 entailment - 이어짐, contradict - 반(反)함, neutral - 관계없음 세 label로 나눕니다. \n",
    "\n",
    "node에서 다룬 것은 정제된 BERT pretrain모델입니다. 이것은 label없이 말뭉치에서 마스크를 씌우고 문장이 이어지는지를 라벨로 하여 학습시킨 것이므로, 두 문장의 관계를 따지는 것이라 연관이 아예 없어보이지는 않는데 안된다니 아쉽네요.  \n",
    "\n",
    "\n",
    "# MNLI dataset 둘러보기\n",
    "\n",
    "tensorflow-datasets 패키지와 포함된 datasets에 리스트에 대해서는 https://www.tensorflow.org/datasets/catalog/overview#all_datasets, \n",
    "그 중 GLUE에 관한 부분은 https://www.tensorflow.org/datasets/catalog/glue 에서 볼 수 있습니다.\n",
    "\n",
    "```Shell\n",
    "pip install tensorflow-datasets -U\n",
    "```\n",
    "glue는 다음과 같은 구조로 돼있습니다.\n",
    "```python\n",
    "FeaturesDict({\n",
    "    'hypothesis': Text(shape=(), dtype=tf.string),\n",
    "    'idx': tf.int32,\n",
    "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
    "    'premise': Text(shape=(), dtype=tf.string),\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6a45478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb8db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d54498c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 298.29 MiB (download: 298.29 MiB, generated: 100.56 MiB, total: 398.85 MiB) to /aiffel/tensorflow_datasets/glue/mnli/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f355e73a884efc8d41a92ba1aeaf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff62ceeadba4a91b4fd6c5d37b671f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba3087e7bca422cade2cfaaa4bcb363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/5 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-train.tfrecord...:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched examples...:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-validation_matched.tfrecord...:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched examples...:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-validation_mismatched.tfrecord...:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_matched examples...:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-test_matched.tfrecord...:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_mismatched examples...:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-test_mismatched.tfrecord...:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /aiffel/tensorflow_datasets/glue/mnli/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds=tfds.load('glue/mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016b33e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfds.load('glue/mnli'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf65a1",
   "metadata": {},
   "source": [
    "STEP 1. mnli 데이터셋을 분석해 보기\n",
    "tensorflow-datasets를 이용하여 glue/mnli를 다운로드하려면 tensorflow-datasets 라이브러리 버전을 올려야 합니다.\n",
    "\n",
    "$ pip install tensorflow-datasets -U\n",
    "위 명령어를 통해 라이브러리 업그레이드를 진행해 주세요!\n",
    "\n",
    "STEP 2. MNLIProcessor클래스 구현하기\n",
    "STEP 3. 위에서 구현한 processor 및 Huggingface에서 제공하는 tokenizer를 활용하여 데이터셋 구성하기\n",
    "STEP 4. model을 생성하여 학습 및 테스트를 진행해 보기\n",
    "💡 힌트\n",
    "혹시 STEP 2의 진행에 어려움을 겪고 계신다면 transformer 프로젝트 내부를 살펴보시면 참고할만한 예시 코드를 찾아볼 수 있을 것입니다. transformers의 공식 github을 참고하는 것도 좋은 방법이에요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c32bf4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([Split('train'), 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25aa130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bc216c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\ttf.Tensor(16399, shape=(), dtype=int32)\ttf.Tensor(b'Meaningful partnerships with stakeholders is crucial.', shape=(), dtype=string)\ttf.Tensor(1, shape=(), dtype=int64)\ttf.Tensor(b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', shape=(), dtype=string)\n",
      "\n",
      "\n",
      "1\ttf.Tensor(206287, shape=(), dtype=int32)\ttf.Tensor(b'The Clinton followers kept to the higher ground in the discussion. ', shape=(), dtype=string)\ttf.Tensor(0, shape=(), dtype=int64)\ttf.Tensor(b'The Clinton surrogates also held the high ground in the context war.', shape=(), dtype=string)\n",
      "\n",
      "\n",
      "2\ttf.Tensor(352707, shape=(), dtype=int32)\ttf.Tensor(b'Women have jobs in all areas of the workforce, they are almost getting the same wages as most men,', shape=(), dtype=string)\ttf.Tensor(1, shape=(), dtype=int64)\ttf.Tensor(b\"um-hum because women are in every field now i mean i can't think of a field that they're not involved in\", shape=(), dtype=string)\n",
      "\n",
      "\n",
      "3\ttf.Tensor(372070, shape=(), dtype=int32)\ttf.Tensor(b'Houston is freezing and dry right now.', shape=(), dtype=string)\ttf.Tensor(2, shape=(), dtype=int64)\ttf.Tensor(b'Houston is really humid now', shape=(), dtype=string)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, doc in enumerate(ds['train']):\n",
    "    print(idx,doc['idx'], doc['hypothesis'], doc['label'], doc['premise'], sep='\\t')\n",
    "    print('\\n')\n",
    "    if idx==3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223fe2e",
   "metadata": {},
   "source": [
    "문장을 몇 개 살펴보니, label이 다음과 같음을 알 수 있습니다.   \n",
    "\n",
    "|Label|0|1|2|\n",
    "|---|---|---|---|\n",
    "|관계|이어짐|무관|반례|\n",
    "\n",
    "그런데 0, 1이 확실하지 않습니다. 잘 모르겠네요. 일단 계속 해봅시다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9f74d",
   "metadata": {},
   "source": [
    "[tensorflow.org, tfds.as_dataframe](https://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe) 을 통해 pandas 데이터프레임을 받을 수 있다고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9578934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split('train'), 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched']\n"
     ]
    }
   ],
   "source": [
    "keys=list(ds.keys())\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f04eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0217013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tfds.as_dataframe(ds[keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e95a45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Meaningful partnerships with stakeholders is...</td>\n",
       "      <td>16399</td>\n",
       "      <td>1</td>\n",
       "      <td>b'In recognition of these tensions, LSC has wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'The Clinton followers kept to the higher gro...</td>\n",
       "      <td>206287</td>\n",
       "      <td>0</td>\n",
       "      <td>b'The Clinton surrogates also held the high gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Women have jobs in all areas of the workforc...</td>\n",
       "      <td>352707</td>\n",
       "      <td>1</td>\n",
       "      <td>b\"um-hum because women are in every field now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Houston is freezing and dry right now.'</td>\n",
       "      <td>372070</td>\n",
       "      <td>2</td>\n",
       "      <td>b'Houston is really humid now'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"But they wouldn't be leaving right now. \"</td>\n",
       "      <td>160184</td>\n",
       "      <td>1</td>\n",
       "      <td>b'But not now.'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hypothesis     idx  label  \\\n",
       "0  b'Meaningful partnerships with stakeholders is...   16399      1   \n",
       "1  b'The Clinton followers kept to the higher gro...  206287      0   \n",
       "2  b'Women have jobs in all areas of the workforc...  352707      1   \n",
       "3          b'Houston is freezing and dry right now.'  372070      2   \n",
       "4        b\"But they wouldn't be leaving right now. \"  160184      1   \n",
       "\n",
       "                                             premise  \n",
       "0  b'In recognition of these tensions, LSC has wo...  \n",
       "1  b'The Clinton surrogates also held the high gr...  \n",
       "2  b\"um-hum because women are in every field now ...  \n",
       "3                     b'Houston is really humid now'  \n",
       "4                                    b'But not now.'  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc28f727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392702"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31531d",
   "metadata": {},
   "source": [
    "용량이 300MB 정도라서 시간이 꽤 걸리므로 일부만 써서 테스트하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae451ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c7ec6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Meaningful partnerships with stakeholders is...</td>\n",
       "      <td>16399</td>\n",
       "      <td>1</td>\n",
       "      <td>b'In recognition of these tensions, LSC has wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'The Clinton followers kept to the higher gro...</td>\n",
       "      <td>206287</td>\n",
       "      <td>0</td>\n",
       "      <td>b'The Clinton surrogates also held the high gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Women have jobs in all areas of the workforc...</td>\n",
       "      <td>352707</td>\n",
       "      <td>1</td>\n",
       "      <td>b\"um-hum because women are in every field now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Houston is freezing and dry right now.'</td>\n",
       "      <td>372070</td>\n",
       "      <td>2</td>\n",
       "      <td>b'Houston is really humid now'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"But they wouldn't be leaving right now. \"</td>\n",
       "      <td>160184</td>\n",
       "      <td>1</td>\n",
       "      <td>b'But not now.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>b'John made me light the gas.'</td>\n",
       "      <td>88879</td>\n",
       "      <td>2</td>\n",
       "      <td>b'John strode across the room, and lit the gas. '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>b\"Music doesn't identify good or bad things. \"</td>\n",
       "      <td>257511</td>\n",
       "      <td>2</td>\n",
       "      <td>b\"yeah i'm kind of thinking that's maybe our g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>b'Lawrence Cavendish woke me up during the nig...</td>\n",
       "      <td>262058</td>\n",
       "      <td>0</td>\n",
       "      <td>b'It seemed to be the middle of the night when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>b'This was from another student at state.'</td>\n",
       "      <td>201141</td>\n",
       "      <td>0</td>\n",
       "      <td>b'well the interesting thing was this was from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>b'Less than half of the colonists did not melt...</td>\n",
       "      <td>305981</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Unlike the other colonists, who eventually m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hypothesis     idx  label  \\\n",
       "0    b'Meaningful partnerships with stakeholders is...   16399      1   \n",
       "1    b'The Clinton followers kept to the higher gro...  206287      0   \n",
       "2    b'Women have jobs in all areas of the workforc...  352707      1   \n",
       "3            b'Houston is freezing and dry right now.'  372070      2   \n",
       "4          b\"But they wouldn't be leaving right now. \"  160184      1   \n",
       "..                                                 ...     ...    ...   \n",
       "995                     b'John made me light the gas.'   88879      2   \n",
       "996     b\"Music doesn't identify good or bad things. \"  257511      2   \n",
       "997  b'Lawrence Cavendish woke me up during the nig...  262058      0   \n",
       "998         b'This was from another student at state.'  201141      0   \n",
       "999  b'Less than half of the colonists did not melt...  305981      1   \n",
       "\n",
       "                                               premise  \n",
       "0    b'In recognition of these tensions, LSC has wo...  \n",
       "1    b'The Clinton surrogates also held the high gr...  \n",
       "2    b\"um-hum because women are in every field now ...  \n",
       "3                       b'Houston is really humid now'  \n",
       "4                                      b'But not now.'  \n",
       "..                                                 ...  \n",
       "995  b'John strode across the room, and lit the gas. '  \n",
       "996  b\"yeah i'm kind of thinking that's maybe our g...  \n",
       "997  b'It seemed to be the middle of the night when...  \n",
       "998  b'well the interesting thing was this was from...  \n",
       "999  b'Unlike the other colonists, who eventually m...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ebfb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load('glue/mnli', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cde4698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'validation_matched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'validation_mismatched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'test_matched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'test_mismatched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "700e9912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    full_name='glue/mnli/2.0.0',\n",
       "    description=\"\"\"\n",
       "    GLUE, the General Language Understanding Evaluation benchmark\n",
       "    (https://gluebenchmark.com/) is a collection of resources for training,\n",
       "    evaluating, and analyzing natural language understanding systems.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    The Multi-Genre Natural Language Inference Corpus is a crowdsourced\n",
       "    collection of sentence pairs with textual entailment annotations. Given a premise sentence\n",
       "    and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis\n",
       "    (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are\n",
       "    gathered from ten different sources, including transcribed speech, fiction, and government reports.\n",
       "    We use the standard test set, for which we obtained private labels from the authors, and evaluate\n",
       "    on both the matched (in-domain) and mismatched (cross-domain) section. We also use and recommend\n",
       "    the SNLI corpus as 550k examples of auxiliary training data.\n",
       "    \"\"\",\n",
       "    homepage='http://www.nyu.edu/projects/bowman/multinli/',\n",
       "    data_path='/aiffel/tensorflow_datasets/glue/mnli/2.0.0',\n",
       "    download_size=298.29 MiB,\n",
       "    dataset_size=100.56 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'hypothesis': Text(shape=(), dtype=tf.string),\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
       "        'premise': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test_matched': <SplitInfo num_examples=9796, num_shards=1>,\n",
       "        'test_mismatched': <SplitInfo num_examples=9847, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=392702, num_shards=1>,\n",
       "        'validation_matched': <SplitInfo num_examples=9815, num_shards=1>,\n",
       "        'validation_mismatched': <SplitInfo num_examples=9832, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{N18-1101,\n",
       "      author = \"Williams, Adina\n",
       "                and Nangia, Nikita\n",
       "                and Bowman, Samuel\",\n",
       "      title = \"A Broad-Coverage Challenge Corpus for\n",
       "               Sentence Understanding through Inference\",\n",
       "      booktitle = \"Proceedings of the 2018 Conference of\n",
       "                   the North American Chapter of the\n",
       "                   Association for Computational Linguistics:\n",
       "                   Human Language Technologies, Volume 1 (Long\n",
       "                   Papers)\",\n",
       "      year = \"2018\",\n",
       "      publisher = \"Association for Computational Linguistics\",\n",
       "      pages = \"1112--1122\",\n",
       "      location = \"New Orleans, Louisiana\",\n",
       "      url = \"http://aclweb.org/anthology/N18-1101\"\n",
       "    }\n",
       "    @article{bowman2015large,\n",
       "      title={A large annotated corpus for learning natural language inference},\n",
       "      author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},\n",
       "      journal={arXiv preprint arXiv:1508.05326},\n",
       "      year={2015}\n",
       "    }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb6518ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/aiffel/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e596e9469e864849ac6f5d0b1011bcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_mnli_dataset = load_dataset('glue', 'mnli')\n",
    "print(huggingface_mnli_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e66a733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datasets.dataset_dict.DatasetDict,\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 392702\n",
       "     })\n",
       "     validation_matched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9815\n",
       "     })\n",
       "     validation_mismatched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9832\n",
       "     })\n",
       "     test_matched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9796\n",
       "     })\n",
       "     test_mismatched: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "         num_rows: 9847\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(huggingface_mnli_dataset), huggingface_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16c1e311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       "  'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him',\n",
       "  'One of our number will carry out your instructions minutely.',\n",
       "  'How do you know? All this is their information again.',\n",
       "  \"yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range\"],\n",
       " 'hypothesis': ['Product and geography are what make cream skimming work. ',\n",
       "  'You lose the things to the following level if the people recall.',\n",
       "  'A member of my team will execute your orders with immense precision.',\n",
       "  'This information belongs to them.',\n",
       "  'The tennis shoes have a range of prices.'],\n",
       " 'label': [1, 0, 0, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_mnli_dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51b593df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(huggingface_mnli_dataset['train'][:5]['premise'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4735b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package datasets:\n",
      "\n",
      "NAME\n",
      "    datasets\n",
      "\n",
      "DESCRIPTION\n",
      "    # flake8: noqa\n",
      "    # coding=utf-8\n",
      "    # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    arrow_dataset\n",
      "    arrow_reader\n",
      "    arrow_writer\n",
      "    builder\n",
      "    combine\n",
      "    commands (package)\n",
      "    config\n",
      "    data_files\n",
      "    dataset_dict\n",
      "    features (package)\n",
      "    filesystems (package)\n",
      "    fingerprint\n",
      "    formatting (package)\n",
      "    hf_api\n",
      "    info\n",
      "    inspect\n",
      "    io (package)\n",
      "    iterable_dataset\n",
      "    keyhash\n",
      "    load\n",
      "    metric\n",
      "    naming\n",
      "    packaged_modules (package)\n",
      "    search\n",
      "    splits\n",
      "    streaming\n",
      "    table\n",
      "    tasks (package)\n",
      "    utils (package)\n",
      "\n",
      "SUBMODULES\n",
      "    deprecation_utils\n",
      "    doc_utils\n",
      "    download_manager\n",
      "    extract\n",
      "    file_utils\n",
      "    filelock\n",
      "    info_utils\n",
      "    logging\n",
      "    mock_download_manager\n",
      "    patching\n",
      "    py_utils\n",
      "    streaming_download_manager\n",
      "    tqdm_utils\n",
      "    typing\n",
      "    version\n",
      "\n",
      "FUNCTIONS\n",
      "    total_allocated_bytes(...)\n",
      "        total_allocated_bytes()\n",
      "        \n",
      "        Return the currently allocated bytes from the default memory pool.\n",
      "        Other memory pools may not be accounted for.\n",
      "\n",
      "DATA\n",
      "    SCRIPTS_VERSION = '1.14.0'\n",
      "    tqdm = <datasets.utils.tqdm_utils._tqdm_cls object>\n",
      "\n",
      "VERSION\n",
      "    1.14.0\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.9/site-packages/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0479f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataProcessor in module transformers.data.processors.utils:\n",
      "\n",
      "class DataProcessor(builtins.object)\n",
      " |  Base class for data converters for sequence classification data sets.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_dev_examples(self, data_dir)\n",
      " |      Gets a collection of :class:`InputExample` for the dev set.\n",
      " |  \n",
      " |  get_example_from_tensor_dict(self, tensor_dict)\n",
      " |      Gets an example from a dict with tensorflow tensors.\n",
      " |      \n",
      " |      Args:\n",
      " |          tensor_dict: Keys and values should match the corresponding Glue\n",
      " |              tensorflow_dataset examples.\n",
      " |  \n",
      " |  get_labels(self)\n",
      " |      Gets the list of labels for this data set.\n",
      " |  \n",
      " |  get_test_examples(self, data_dir)\n",
      " |      Gets a collection of :class:`InputExample` for the test set.\n",
      " |  \n",
      " |  get_train_examples(self, data_dir)\n",
      " |      Gets a collection of :class:`InputExample` for the train set.\n",
      " |  \n",
      " |  tfds_map(self, example)\n",
      " |      Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
      " |      examples to the correct format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "\n",
    "help(DataProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f1602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
