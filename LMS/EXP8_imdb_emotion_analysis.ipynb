{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chhyyi/aiffel/blob/main/LMS/EXP8_imdb_emotion_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzXz7ZAJBlj_"
      },
      "source": [
        "이 노트북은 modulab의 lms exploration node, imdb감정분석 노트북입니다.\n",
        "\n",
        "\n",
        "# 2. 네이버 영화 리뷰 감성분석 (8-11 프로젝트)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.(1). 데이터 준비와 확인"
      ],
      "metadata": {
        "id": "Vbs43oTeXFr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "google colab에서 쓰기 위해 mecab 설치 : 아무튼 이걸 하니까 됩니다.\n",
        "참고로 mecab은 원래 일본 전통과자라는 것 같습니다. konlpy 공식 홈페이지로 보이는 곳에서 다운받았습니다만 업데이트가 몇 년 전에 종료됐습니다. 앞으로도 괜찮을까요? 아무튼 일단은 작동합니다.\n",
        "```python\n",
        "!apt-get update && apt-get install -y g++ default-jdk\n",
        "!pip install konlpy\n",
        "!sudo apt-get install curl git\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "```"
      ],
      "metadata": {
        "id": "5uUcNx_wFS84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXv9kpti-K8R"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HrxfLf9g1LB",
        "outputId": "66dff8cc-70fe-4ab3-8867-89a4909e9eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "1.3.5\n",
            "0.6.0\n",
            "3.6.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install konlpy\n",
        "import konlpy\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "print(pd.__version__)\n",
        "print(konlpy.__version__)\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "버전이 조금씩 다르지만 크게 다르지는 않습니다. 평소에도 그랬듯 잘 작동하길 기대하면서 일단 진행합니다. "
      ],
      "metadata": {
        "id": "tOZmqR37XQZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1U3DXEzZCTHZ",
        "outputId": "a2b360fa-26f1-4683-ed8c-774f8732df2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85df31ca-f0ae-457d-beb7-b4329c8db103\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85df31ca-f0ae-457d-beb7-b4329c8db103')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85df31ca-f0ae-457d-beb7-b4329c8db103 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85df31ca-f0ae-457d-beb7-b4329c8db103');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#import pandas as pd\n",
        "\n",
        "train_data = pd.read_table('/content/drive/MyDrive/colabdata/modulabs/lms_exp8/ratings_train.txt')\n",
        "test_data = pd.read_table('/content/drive/MyDrive/colabdata/modulabs/lms_exp8/ratings_test.txt')\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "google drive에 옮겨둔 파일을 로드합니다. 잘 읽어왔습니다.\n",
        "\n",
        "## 2.(2) 데이터로더 구성\n",
        "데이터로더가 너무 길어졌습니다. 뒤늦게 counter의 존재를 알아버렸네요."
      ],
      "metadata": {
        "id": "dDuwTLbhXfmH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uC8zHvWC9ex",
        "outputId": "66da7bc9-d6f8-4615-8120-6ae979ac907c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 2, 3, 3, 4, 5, 6, 7, 8], [9, 3, 10, 11, 12, 13, 14, 15, 3, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26], [27, 28, 29, 3, 3, 30, 31, 32, 33, 3, 3, 34, 35], [36, 37, 38, 18, 39, 40, 14, 41, 42, 43, 44, 45, 46, 47, 48, 49, 40, 50, 51, 52, 53, 54, 33]]\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "num_words=10000\n",
        "tokenizer = Mecab()\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "\n",
        "\n",
        "def load_data(train_data, test_data, num_words=num_words):\n",
        "    train_data.drop_duplicates(inplace=True)\n",
        "    test_data.drop_duplicates(inplace=True)\n",
        "    train_data.dropna(inplace=True)\n",
        "    test_data.dropna(inplace=True)\n",
        "\n",
        "    train_idx=[]\n",
        "    test_idx=[]\n",
        "    \n",
        "    dict_last_index=0\n",
        "\n",
        "    appearance={\"<PAD>\":100,\"<BOS>\":100, \"<UNK>\":100}\n",
        "    #appearance: 전체 데이터에서 등장 횟수.\n",
        "    #인덱스는 그냥 word로 하겠습니다.\n",
        "    \n",
        "    #word to_index, index_to_word 초기화.\n",
        "    word_to_index={\"<PAD>\":0, \"<BOS>\":1, \"<UNK>\":2}\n",
        "    index_to_word={0:\"<PAD>\", 1:\"<BOS>\", 2:\"<UNK>\"}\n",
        "\n",
        "    for sentence in train_data['document']:\n",
        "        tokens=tokenizer.morphs(sentence)\n",
        "        token_to_index=[]\n",
        "        \n",
        "        for tk in tokens:\n",
        "            if tk not in stopwords:\n",
        "                if tk in word_to_index.keys():\n",
        "                    token_to_index.append(word_to_index[tk])\n",
        "                    appearance[tk]+=1\n",
        "\n",
        "                else:\n",
        "                    index_to_word[dict_last_index]=tk\n",
        "                    word_to_index[tk]=dict_last_index\n",
        "                    dict_last_index+=1\n",
        "                    token_to_index.append(word_to_index[tk])\n",
        "                    appearance[tk]=1\n",
        "\n",
        "        train_idx.append(token_to_index)\n",
        "\n",
        "    for sentence in test_data['document']:\n",
        "        tokens=tokenizer.morphs(sentence)\n",
        "        token_to_index=[]\n",
        "        for tk in tokens:\n",
        "            if tk not in stopwords:\n",
        "                if tk in word_to_index.keys():\n",
        "                    token_to_index.append(word_to_index[tk])\n",
        "                    appearance[tk]+=1\n",
        "                else:\n",
        "                    index_to_word[dict_last_index]=tk\n",
        "                    word_to_index[tk]=dict_last_index\n",
        "                    dict_last_index+=1\n",
        "                    token_to_index.append(word_to_index[tk])\n",
        "                    appearance[tk]=1\n",
        "\n",
        "        test_idx.append(token_to_index)\n",
        "    \n",
        "    if len(index_to_word)>num_words:\n",
        "        min=np.min(appearance.values())\n",
        "        for word, index in word_to_index.items():\n",
        "            if appearance[word]==min:\n",
        "                del word_to_index[word] \n",
        "                del index_to_word[index]\n",
        "            if len(index_to_word)>num_words:\n",
        "                break\n",
        "    \n",
        "    \n",
        "    for i, sentence in enumerate(test_idx):\n",
        "        for j, token in enumerate(sentence):\n",
        "            if token not in index_to_word.keys():\n",
        "                test_idx[i,j]=word_to_index[\"<UNK>\"]\n",
        "    for i, sentence in enumerate(train_idx):\n",
        "        for j, token in enumerate(sentence):\n",
        "            if token not in index_to_word.keys():\n",
        "                train_idx[i,j]=word_to_index[\"<UNK>\"]\n",
        "\n",
        "        \n",
        "    return train_idx, train_data['label'], test_idx, test_data['label'], word_to_index, index_to_word\n",
        "\n",
        "#index_to_word도 load_data에서 만들었기 때문에 그냥 같이 반환합니다.\n",
        "X_train, y_train, X_test, y_test, word_to_index, index_to_word = load_data(train_data, test_data)\n",
        "print(X_train[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hur9R1zuDABd"
      },
      "outputs": [],
      "source": [
        "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
        "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
        "\n",
        "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
        "def get_encoded_sentences(sentences, word_to_index):\n",
        "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
        "\n",
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
        "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_decoded_sentence(X_train[4], index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "872H3Uj8OawA",
        "outputId": "e79c4078-5160-4742-f95b-7b9d2de914b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'익살 스런 연기 돋보였 던 영화 ! 스파이더맨 에서 늙 어 보이 기 만 했 던 커스틴 던스트 너무나 이뻐 보였 다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터로더가 잘 작동하는 것 같습니다. \n",
        "\n",
        "## 2.(3). 모델 구성을 위한 데이터 분석 및 가공\n",
        "평균$+2\\sigma$으로 자르겠습니다."
      ],
      "metadata": {
        "id": "fxBln-HGWfCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length=[len(sentence) for sentence in X_train]\n",
        "print(np.max(length), np.mean(length), np.std(length))\n",
        "maxlen=np.mean(length)*np.std(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DivaWW1OjJag",
        "outputId": "6a56e1a3-abe4-4179-bc43-e726d46fc959"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116 15.644794826494216 12.841204643969176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_to_index[\"<PAD>\"],\n",
        "                                                        padding='pre',\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_to_index[\"<PAD>\"],\n",
        "                                                       padding='pre',\n",
        "                                                       maxlen=maxlen)\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7_rGkIxlwwS",
        "outputId": "9ef48b94-d88e-4c01-89df-9fdb3d59b480"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(149995, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test=np.ndarray(X_test)\n",
        "x_train=np.ndarray(X_train)\n",
        "print(x_test, x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WWxDD6_8meJM",
        "outputId": "249c3d87-daff-4422-cc3c-65062324da2e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-79cafbbd65c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: maximum supported dimension for an ndarray is 32, found 49997"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.(4). 모델 구성 및 validation set 구성"
      ],
      "metadata": {
        "id": "tRdh5nglmB_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "26H7E9FQmBQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.(5). 모델 훈련"
      ],
      "metadata": {
        "id": "_-jFI9rsmGUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.(6). Loss, Accuracy 그래프 시각화\n"
      ],
      "metadata": {
        "id": "KfizmvmHmJb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.(6). 학습된 embedding 레이어 분석\n"
      ],
      "metadata": {
        "id": "wsOWezeomQjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.(6). 한국어 Word2Vec임베딩 활용하여 성능 개선\n"
      ],
      "metadata": {
        "id": "SG2S9aRzmRNd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyY-xqrhb2rT"
      },
      "source": [
        "\n",
        "\n",
        "# 1. 프로젝트 이전 코드\n",
        "lms에서 코드를 좀 가져오겠습니다.\n",
        "-> mecab 설치 너무 번거롭네요. 그냥 lms의 주피터 노트북으로 갑시다 -> 다시 여기서 하기로 하였습니다.  \n",
        "## 1.1. encode, decode함수\n",
        "단어를 index(숫자)로 저장했다가 다시 문장으로 해석했다 하는 과정입니다. 여러개 처리하는 것을 function 둘로 나눠놨네요. for loop 표현식을 쓰고 있습니다.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
        "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
        "\n",
        "#print(get_encoded_sentence('i eat lunch', word_to_index))\n",
        "\n",
        "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
        "def get_encoded_sentences(sentences, word_to_index):\n",
        "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
        "\n",
        "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
        "\n",
        "#encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
        "#print(encoded_sentences)\n",
        "```\n",
        "\n",
        "```python\n",
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "#print(get_decoded_sentence([1, 3, 4, 5], index_to_word))\n",
        "\n",
        "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
        "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
        "\n",
        "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
        "#print(get_decoded_sentences(encoded_sentences, index_to_word))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok7Qlpp2d0uC"
      },
      "source": [
        "## 1.2. IMDB데이터셋 가져오기\n",
        "tf.keras에 서 데이터를 가져옵니다. 인코딩 된 것이라 사전도 같이 줍니다.\n",
        "\n",
        "```python\n",
        "imdb = tf.keras.datasets.imdb\n",
        "\n",
        "# IMDb 데이터셋 다운로드 \n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))\n",
        "\n",
        "\n",
        "word_to_index = imdb.get_word_index()\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}\n",
        "\n",
        "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
        "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
        "\n",
        "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
        "word_to_index[\"<PAD>\"] = 0\n",
        "word_to_index[\"<BOS>\"] = 1\n",
        "word_to_index[\"<UNK>\"] = 2  # unknown\n",
        "word_to_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}\n",
        "\n",
        "#print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
        "#print(word_to_index['the'])  # 4 이 출력됩니다. \n",
        "#print(index_to_word[4])     # 'the' 가 출력됩니다.\n",
        "\n",
        "total_data_text = list(x_train) + list(x_test)\n",
        "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
        "num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "num_tokens = np.array(num_tokens)\n",
        "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
        "print('문장길이 평균 : ', np.mean(num_tokens))\n",
        "print('문장길이 최대 : ', np.max(num_tokens))\n",
        "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
        "\n",
        "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "maxlen = int(max_tokens)\n",
        "print('pad_sequences maxlen : ', maxlen)\n",
        "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))\n",
        "\n",
        "#padding: pre 혹은 post로\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        value=word_to_index[\"<PAD>\"],\n",
        "                                                        padding='post', # 혹은 'pre'\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                       value=word_to_index[\"<PAD>\"],\n",
        "                                                       padding='post', # 혹은 'pre'\n",
        "                                                       maxlen=maxlen)\n",
        "\n",
        "print(x_train.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUbJKytKgkDL"
      },
      "source": [
        "## 1.3. RNN 모델 \n",
        "```python\n",
        "vocab_size = 10000\n",
        "word_vector_dim = 16  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(tf.keras.layers.LSTM(128))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "```\n",
        "```python\n",
        "# validation set 10000건 분리\n",
        "x_val = x_train[:10000]   \n",
        "y_val = y_train[:10000]\n",
        "\n",
        "# validation set을 제외한 나머지 15000건\n",
        "partial_x_train = x_train[10000:]  \n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "print(partial_x_train.shape)\n",
        "print(partial_y_train.shape)\n",
        "```\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val), verbose=1)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zyY-xqrhb2rT"
      ],
      "name": "EXP8_imdb emotion analysis.ipynb",
      "provenance": [],
      "mount_file_id": "1lv3FDw2hKe_Gp56mzCxGPIHof8p0m6Cz",
      "authorship_tag": "ABX9TyMvqHcx2Ir00aX4bIa8cLZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}