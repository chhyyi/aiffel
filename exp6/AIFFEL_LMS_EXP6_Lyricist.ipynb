{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chhyyi/aiffel/blob/main/exp6/AIFFEL_LMS_EXP6_Lyricist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtKHv7V-QuI8"
      },
      "source": [
        "# Lyricist\n",
        "Aiffel LMS exploration node 6 (by modulabs), lyricist 만들기\n",
        "\n",
        "## Step1 데이터 다운로드, step2 데이터 불러오기\n",
        "- 데이터셋을 불러오기 위해 구글 드라이브 마운트합니다.\n",
        "- 미리 다운로드를 받아서 구글 드라이브에 저장해두었습니다. colab으로 매번 옮기기가 번거롭기는 하지만 공유하기가 이쪽이 편하니 여기서 하겠습니다.  \n",
        "- 버전을 확인해보면 2.8.2.가 나오는데 큰 차이가 없기를 기대해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ84WPbESDyv",
        "outputId": "1df45db1-a136-4318-b0c5-daf723fa5a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adele.txt\t       dolly-parton.txt        ludacris.txt\n",
            "al-green.txt\t       drake.txt\t       michael-jackson.txt\n",
            "alicia-keys.txt        dr-seuss.txt\t       missy-elliott.txt\n",
            "amy-winehouse.txt      eminem.txt\t       nickelback.txt\n",
            "beatles.txt\t       janisjoplin.txt\t       nicki-minaj.txt\n",
            "bieber.txt\t       jimi-hendrix.txt        nirvana.txt\n",
            "bjork.txt\t       johnny-cash.txt\t       notorious_big.txt\n",
            "blink-182.txt\t       joni-mitchell.txt       notorious-big.txt\n",
            "bob-dylan.txt\t       kanye.txt\t       nursery_rhymes.txt\n",
            "bob-marley.txt\t       kanye-west.txt\t       patti-smith.txt\n",
            "britney-spears.txt     Kanye_West.txt\t       paul-simon.txt\n",
            "bruce-springsteen.txt  lady-gaga.txt\t       prince.txt\n",
            "bruno-mars.txt\t       leonard-cohen.txt       radiohead.txt\n",
            "cake.txt\t       lil-wayne.txt\t       rihanna.txt\n",
            "dickinson.txt\t       Lil_Wayne.txt\t       r-kelly.txt\n",
            "disney.txt\t       lin-manuel-miranda.txt\n",
            "dj-khaled.txt\t       lorde.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/colabdata/modulabs/lms_exp6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zgCsl66MTJXv",
        "outputId": "ab6853b2-177f-4a76-833d-48c4be8e7f0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orL0KhECSUeT",
        "outputId": "04c0929c-d932-4a19-df7e-3b936ac9765f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 186966\n",
            "Examples:\n",
            " ['Jolene, Jolene, Jolene, Jolene', \"I'm begging of you please don't take my man\", 'Jolene, Jolene, Jolene, Jolene']\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/colabdata/modulabs/lms_exp6/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyJrCJiXTCKd"
      },
      "source": [
        "## Step 3. 데이터 정제\n",
        "토큰 개수가 15개를 넘는 문장은 제외하라고 권장하고 있습니다. preprocess_sentence()를 사용하라고 하므로 일단 함수를 가져옵니다. 함수에 대한 코멘트도 그대로 가져왔습니다.\n",
        "> 입력된 문장을\n",
        "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "2. 특수문자 양쪽에 공백을 넣고\n",
        "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "5. 다시 양쪽 공백을 지웁니다\n",
        "6. 문장 시작에는 \\<start>, 끝에는 \\<end>를 추가합니다  \n",
        "이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVeym3JMSjbR"
      },
      "outputs": [],
      "source": [
        "import re \n",
        "import numpy as np\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpT5fixpUtaT",
        "outputId": "faafd44a-66af-4479-9f03-545c5c008931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> l o v e l e s s <end>\n"
          ]
        }
      ],
      "source": [
        "print(preprocess_sentence(\"L-O-V-E-L-E-S-S\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVW4Fs4dcJt3"
      },
      "source": [
        "바로 반복되는 문장도 지워버렸습니다. 문장 단위의 sequence는 어차피 안보니까 상관 없다고 판단했습니다.  \n",
        "띄어쓰기 15개 이상으로 구분되는 string도 지웁니다. 토큰화 했을 때 지우라고 했는데 그냥 여기서 처리했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyQOiprlUv10",
        "outputId": "dfcd1ae2-7bc7-4ef5-c0f6-373df77c96df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> jolene , jolene , jolene , jolene <end>',\n",
              " '<start> i m begging of you please don t take my man <end>',\n",
              " '<start> jolene , jolene , jolene , jolene <end>',\n",
              " '<start> with flaming locks of auburn hair <end>',\n",
              " '<start> with ivory skin and eyes of emerald green <end>',\n",
              " '<start> your smile is like a breath of spring <end>',\n",
              " '<start> your voice is soft like summer rain <end>',\n",
              " '<start> and i cannot compete with you <end>',\n",
              " '<start> jolene he talks about you in his sleep <end>',\n",
              " '<start> and there s nothing i can do to keep <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "corpus = []\n",
        "dummy=''\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0 or dummy==sentence: continue #바로 반복되는 같은 문장도 지워버렸습니다.\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    if len(preprocessed_sentence.split(sep=' '))>15: continue #띄어쓰기 15개 이상으로 구분되는 string도 지웁니다.\n",
        "    corpus.append(preprocessed_sentence)\n",
        "    dummy=sentence\n",
        "        \n",
        "#check up of refined data\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8JFceZnXFB6"
      },
      "source": [
        "해석 불가능한 가사가 나와서 조금 살펴봅니다.\n",
        "- it's, He's 를 it s, He s와 같이 두 토큰으로 변환하는 것이 괜찮을까요? 큰 문제는 없을 것 같습니다.\n",
        "- 일부 파일에서 제목으로 보이는 것들이 발견됩니다만 일단 그냥 두겠습니다.\n",
        "- 반복되는 문장이 상당히 많습니다.\n",
        "- L-O-V-E-L-E-S-S 는 괜찮을까요? 이건 l o v e l e s s 8개의 토큰이 될 것 같습니다.\n",
        "- Bruno Mars의 경우 일부 곡에 스페인어를 가사에 쓴 것 같습니다. 일단 그 부분만 임의로 삭제하겠습니다. 그냥 파일에서 지웠습니다.  \n",
        "빈 줄과 반복되는 줄, 15개가 넘는 띄워쓰기로 구분된 문장을 지운 결과 raw_corpus와 corpus의 길이가 187088, 151932으로 만큼 차이가 납니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aya7pBhVUgs",
        "outputId": "cd784e15-062d-4e4d-bf99-a28946596097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151825 186966\n"
          ]
        }
      ],
      "source": [
        "print(len(corpus), len(raw_corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnYGQDrdHzG"
      },
      "source": [
        "## Step4. 평가 데이터셋 분리\n",
        "단어장의 크기는 12000이상으로 하라고 하니까 13000으로 했습니다. 나머지는 \\<unk>라고 해버립니다. padding은 길이를 맞춰주는 겁니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Krd3jgRWVw4",
        "outputId": "a9b7000e-1ee5-4686-a0da-5b9b0b84578b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2 1289    5 ...    0    0    0]\n",
            " [   2    4   25 ...    3    0    0]\n",
            " [   2 1289    5 ...    0    0    0]\n",
            " ...\n",
            " [   2   48   48 ...    0    0    0]\n",
            " [   2    4   25 ...    0    0    0]\n",
            " [   2   48   48 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa8b8204c50>\n"
          ]
        }
      ],
      "source": [
        "#copied code from lms\n",
        "def tokenize(corpus):\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=13000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjtJ_WFzeUKH",
        "outputId": "e18bccc2-a5c3-4f46-df7f-9350c171471b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2 1289    5 1289    5 1289    5 1289    3    0    0    0    0    0]\n",
            "[1289    5 1289    5 1289    5 1289    3    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "#copied code from lms\n",
        "src_input = tensor[:, :-1] #source에서 <end>를 제거\n",
        "tgt_input = tensor[:, 1:] #target에서 <start>를 제거\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I11uVqUceyuo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjnj5ymgj6oY",
        "outputId": "cbf01e4d-cf99-4b90-d43e-811e45c7841a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>\n",
            "\n",
            " dataset_eval\n",
            "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "#from copied code from lms, make two dataset of train and validation.\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset_train)\n",
        "\n",
        "BUFFER_SIZE = len(dec_train)\n",
        "steps_per_epoch = len(dec_train) // BATCH_SIZE\n",
        "\n",
        "dataset_eval = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "dataset_eval = dataset_eval.shuffle(BUFFER_SIZE)\n",
        "dataset_eval = dataset_eval.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print('\\n dataset_eval')\n",
        "print(dataset_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_watJhEhfno"
      },
      "source": [
        "## step5. 인공지능 만들기\n",
        "> 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
        "\n",
        "이번에도 6-5의 코드를 가져와서 만들어보겠습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhOe72WYhfOL"
      },
      "outputs": [],
      "source": [
        "#copied code from lms\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 1024\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zetUtqDtmjzu"
      },
      "source": [
        "모델을 확인하기 위해 데이터셋에서 하나만 뽑아서 넣어봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze5J8lG1h4VU",
        "outputId": "4d932fe0-e6a1-47ca-8071-fc264be97332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 13001), dtype=float32, numpy=\n",
              "array([[[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [-6.4160943e-04,  4.6776913e-04,  1.1165556e-03, ...,\n",
              "         -2.1839657e-04, -5.6659634e-04,  5.1304145e-04],\n",
              "        [-1.1225600e-03,  6.5918185e-04,  1.7859292e-03, ...,\n",
              "         -3.8471975e-04, -7.5735041e-04,  3.0967922e-04],\n",
              "        ...,\n",
              "        [-1.8769091e-04, -2.3470669e-04,  2.3774614e-03, ...,\n",
              "         -1.1077181e-03, -8.1038492e-04,  2.3716381e-03],\n",
              "        [ 4.5113734e-04, -5.9437245e-04,  3.6216537e-03, ...,\n",
              "         -6.6630135e-04, -1.3476746e-04,  3.4521229e-03],\n",
              "        [ 1.0500229e-03, -9.0237206e-04,  4.8800646e-03, ...,\n",
              "         -2.3464498e-04,  5.6878943e-04,  4.4592666e-03]],\n",
              "\n",
              "       [[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [ 1.1420773e-05,  4.5288651e-04,  3.9101724e-04, ...,\n",
              "         -1.2631807e-04, -6.9180678e-04, -3.8283298e-04],\n",
              "        [-2.7693334e-04,  8.6687168e-04,  5.7242386e-04, ...,\n",
              "         -1.5873545e-04, -9.6136885e-04, -4.8716785e-04],\n",
              "        ...,\n",
              "        [ 1.3792673e-03, -1.7371261e-03,  6.8955515e-03, ...,\n",
              "          1.4283067e-03,  8.2013250e-04,  3.1383771e-03],\n",
              "        [ 1.8530827e-03, -1.9421915e-03,  7.9090558e-03, ...,\n",
              "          1.5902873e-03,  1.4719088e-03,  4.0699295e-03],\n",
              "        [ 2.2323038e-03, -2.0899642e-03,  8.8201724e-03, ...,\n",
              "          1.7267263e-03,  2.1015913e-03,  4.8740399e-03]],\n",
              "\n",
              "       [[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [ 4.7970756e-05,  3.2303335e-05,  5.3807982e-04, ...,\n",
              "          8.7816617e-05, -2.9056153e-04,  3.0569246e-04],\n",
              "        [ 2.8874076e-09, -2.9411254e-04,  5.1833835e-04, ...,\n",
              "          1.8264762e-04,  3.2856929e-04,  7.9098856e-05],\n",
              "        ...,\n",
              "        [ 5.2047987e-04,  7.0172647e-04,  3.0166737e-04, ...,\n",
              "          1.8076624e-03,  9.8007719e-04, -1.3370243e-03],\n",
              "        [ 4.9184018e-04,  5.8432051e-04,  3.4770172e-04, ...,\n",
              "          1.7031112e-03,  1.1310853e-03, -1.3635835e-03],\n",
              "        [ 2.9403094e-04,  9.6963026e-04,  4.8564858e-04, ...,\n",
              "          1.6251821e-03,  1.1673594e-03, -1.0664692e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [-6.4160943e-04,  4.6776913e-04,  1.1165556e-03, ...,\n",
              "         -2.1839657e-04, -5.6659634e-04,  5.1304145e-04],\n",
              "        [-7.2310364e-04,  6.2212907e-04,  1.2431764e-03, ...,\n",
              "         -2.1226495e-05, -5.5485719e-04,  9.3803933e-04],\n",
              "        ...,\n",
              "        [ 2.0843379e-03, -1.7278715e-03,  7.0204129e-03, ...,\n",
              "          1.6535831e-03,  2.8671303e-03,  5.0778659e-03],\n",
              "        [ 2.3828174e-03, -1.9003056e-03,  7.9833241e-03, ...,\n",
              "          1.8268151e-03,  3.3379723e-03,  5.6811227e-03],\n",
              "        [ 2.6151899e-03, -2.0260634e-03,  8.8517759e-03, ...,\n",
              "          1.9585881e-03,  3.7766278e-03,  6.1900294e-03]],\n",
              "\n",
              "       [[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [ 3.4800556e-04,  4.1692430e-04,  1.8077261e-04, ...,\n",
              "         -3.7641247e-04, -6.1190833e-04,  2.9033376e-04],\n",
              "        [ 4.8263569e-04,  3.2731317e-04,  3.0469889e-04, ...,\n",
              "         -4.0728072e-04, -7.9185219e-04,  1.9156454e-04],\n",
              "        ...,\n",
              "        [ 1.9614061e-03, -1.1435737e-03,  3.3202639e-03, ...,\n",
              "          7.7421422e-04,  1.2963614e-03,  2.9623352e-03],\n",
              "        [ 2.4193705e-03, -1.4697213e-03,  4.6422333e-03, ...,\n",
              "          8.7044499e-04,  1.6046886e-03,  3.9065839e-03],\n",
              "        [ 2.8099834e-03, -1.7135722e-03,  5.9030233e-03, ...,\n",
              "          9.6993498e-04,  1.9689985e-03,  4.7579701e-03]],\n",
              "\n",
              "       [[ 4.4230157e-05,  6.3468593e-05,  1.3292658e-04, ...,\n",
              "         -6.3161977e-05, -2.6610674e-04,  4.5868641e-05],\n",
              "        [ 9.4373936e-05, -1.5342505e-04,  7.0518501e-05, ...,\n",
              "         -4.9189565e-04, -2.4916546e-04,  2.3178427e-04],\n",
              "        [ 3.0313228e-04, -5.1965768e-04, -1.9770737e-04, ...,\n",
              "         -6.4733595e-04, -1.5771303e-04,  3.7526042e-04],\n",
              "        ...,\n",
              "        [ 8.3328190e-04, -3.8047633e-03,  4.1878503e-03, ...,\n",
              "         -3.5741553e-04,  1.2258586e-03,  3.7211413e-03],\n",
              "        [ 1.3708274e-03, -3.8834645e-03,  5.5875541e-03, ...,\n",
              "          1.9023106e-05,  1.6188659e-03,  4.6251309e-03],\n",
              "        [ 1.8559764e-03, -3.8653407e-03,  6.8657319e-03, ...,\n",
              "          3.6371665e-04,  2.0626981e-03,  5.4506464e-03]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "#copied code from lms\n",
        "for src_sample, tgt_sample in dataset_train.take(1): break\n",
        "\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOIHqbEh5MK",
        "outputId": "ea4a1adb-d540-45da-d478-a29e8dab769d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  13313024  \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               multiple                  25174016  \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  26639049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,688,713\n",
            "Trainable params: 98,688,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "#copied code from lms\n",
        "\n",
        "model.summary()\n",
        "tf.test.is_gpu_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMKkahgWlnfX",
        "outputId": "f0da7b62-accf-4c78-e528-d463e8d84cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "474/474 [==============================] - 143s 297ms/step - loss: 3.2754 - val_loss: 2.9106\n",
            "Epoch 2/6\n",
            "474/474 [==============================] - 139s 292ms/step - loss: 2.7379 - val_loss: 2.6678\n",
            "Epoch 3/6\n",
            "474/474 [==============================] - 139s 292ms/step - loss: 2.4218 - val_loss: 2.4935\n",
            "Epoch 4/6\n",
            "474/474 [==============================] - 139s 292ms/step - loss: 2.1067 - val_loss: 2.3568\n",
            "Epoch 5/6\n",
            "474/474 [==============================] - 139s 292ms/step - loss: 1.8067 - val_loss: 2.2545\n",
            "Epoch 6/6\n",
            "474/474 [==============================] - 138s 292ms/step - loss: 1.5382 - val_loss: 2.1955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa7c6455310>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "#copied code from lms\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "model.fit(dataset_train, epochs=6, validation_data=(enc_val,dec_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJKo-t5hnJ9v"
      },
      "source": [
        "## val_loss의 평가\n",
        "위에서 나오는 loss는 train data에 대한 것이고 validation data에 대한 loss를 구해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JsBHHn1ymqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfb4ef5-c8af-4d91-c892-be581453552c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "model.metrics_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zSrmU0oNil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7d3196-de95-478b-cfe6-0fcdcde014fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "949/949 [==============================] - 21s 22ms/step - loss: 2.1955\n",
            "eval_loss: 2.1955366134643555 \n"
          ]
        }
      ],
      "source": [
        "eval_loss = model.evaluate(enc_val,dec_val)\n",
        "print('eval_loss: {} '.format(eval_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('',embedding_size, hidden_size,'', eval_loss,'', sep='|') #markdown 표에 붙여놓기 쉽게 해봅니다. 그런데 train_loss랑 step당 걸리는 시간은 못찾겠네요."
      ],
      "metadata": {
        "id": "OafhlMCXw216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88522c3-d1ce-4cc3-c6a2-11762bd5287f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|1024|2048||2.1955366134643555|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSSiKCPmwtN"
      },
      "source": [
        "이 단계까지를 반복하면서 적당한 embedding, hidden size를 찾아서 10 epoch 안에 val_loss를 2.2 밑으로 줄여보라고 합니다. 그러므로 epoch = 10의 결과를 간단히 정리하겠습니다.\n",
        "*처음에 실수로 30epoch까지 돌렸던 결과입니다. overfitting이 됐다고 짐작할 수 있을 것 같습니다. train_loss는 안적어놔서 모르는데 0.9정도 였던 거 같습니다.  \n",
        "embedding size와 hidden size를 늘리기 시작하자, train_loss가 굉장히 작아지기 시작했습니다. 이것은 overfitting 되고 있다는 뜻일까요?\n",
        "\n",
        "|embedding size|hidden size|train_loss|eval_loss|time/step|epochs|\n",
        "|-----|----|----|---|---|---|\n",
        "|256|1024|?|*2.6430249214172363|?|30|\n",
        "|256|1024|2.2778|2.579704999923706|?|10|\n",
        "|128|512|2.6172|2.7422351837158203|?|10|\n",
        "|512|2048|1.2204|2.21757173538208|240ms|10|\n",
        "|1024|2048|0.9981|2.2050082683563232|252ms|10|\n",
        "|512|4096|1.0186|2.238615036010742|741ms|10|\n",
        "|1024|1024|2.0810|2.481119155883789|104ms|10|\n",
        "|512|2048|2.2549|2.484931707382202|241ms|5|\n",
        "|512|2048|1.6797|2.3204803466796875|241ms|8|\n",
        "|512|2048|1.3387|2.2849111557006836|241ms|10|\n",
        "|1024|2048|1.7487|2.2373404502868652|252ms|5|\n",
        "\n",
        "embedding size 1024, hidden size 2048인 경우에 대해서 집중하겠습니다.  \n",
        "\n",
        "|epochs|train_loss|eval_loss|  \n",
        "|----|----|----|  \n",
        "|5|1.7487|2.2373404502868652|\n",
        "|6|1.5201|2.1752|\n",
        "|7|1.2722|2.1769|\n",
        "|8|1.1286|2.1950|\n",
        "|9|1.0452|2.2182|\n",
        "|10|1.0023|2.2417|\n",
        "\n",
        "여기서 overfitting이 일어나고 있는 것 같습니다.\n",
        "epoch 7인 경우에서 작사를 시켜보겠습니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsjRL2D1mMP-"
      },
      "outputs": [],
      "source": [
        "# copied from lms node\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        predict = model(test_tensor) \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> Say\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BO42jzzRzGjS",
        "outputId": "865bb9c2-17e5-4105-986b-21aac6ad1ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> say it all say it all , <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "\n",
        "## 런타임 유형과 나의 과거\n",
        "수동으로 grid search와 같은 일을 엄청나게 해버렸다. 처음으로 꽤 긴 시간이 걸리는 계산을 돌려야 했다. 옛날 기억들이 떠올랐다. 연구실 자리에서, 나는 바로 뒤에 있는 컴퓨터에 원격으로 접속해서 이런저런 계산을 돌리고 그 결과를 기다리며 할 다른 일을 찾아야 했다. 그래서 게임을 많이 했다.  \n",
        " 나는 이 노트북을 실행시키기 위해 colab 노트북의 런타임 유형을 처음으로 변경해야 했다. 지금까지 자동으로 변경됐던 경우도 있었을까? 결국 계산자원(computing power)에 대한 문제가 남아있다. 얼마 전에 아는 사람에게 듣자하니 구글에서는 TPU research cloud를 무료로 사용하게 해주는 프로그램을 진행하고 있다고 한다. 회사나 단체 등에서 지원하면 심사해서 쓰게 해주는 것 같다. 왜 그런 걸 진작에 알지 못했을까?   \n",
        "참 생각이 많아지는 문제다. 어떤 아는 사람에게 캐글 competition에서 돈을 벌 수 있냐고 묻자 '상위권 가려면 컴퓨팅 파워가 좋아야 한다'라고 답했다.  \n",
        "짧은 경험이지만 계산용 컴퓨터를 관리하는 것은 끔찍했다. 나는 결국 GPU를 붙이자는 본래 계획을 폐기하고 상당부분을 부품 교체에 써야 했다. 계산용으로 128GB램에 제온 cpu를 붙여서 구매한 그 녀석은 채 1년이 지나 삐걱거리기 시작했고, 나는 결국 보통 PC처럼 껏다 켰다 하면서 써야 했다. 그런데 계획대로 GPU를 썼을 때 얼마나 성능향상이 있을지는 전혀 알 수 없는 상태였기 때문에 오히려 다행이었는지도 모르겠다.  \n",
        "\n",
        "### 단어장의 크기 등\n",
        " 이 부분은 12000이상으로 하라고 하여서 13000으로 그냥 고정시켰는데 이게 맞을까? 다르게 했다면 어땠을까? 또 중복문장을 제거하는 것이 의미가 있는 처리였을까? 가사 일부를 지우는 등 임의로 처리한 부분도 있는데 이것은 어떨까? 노래 가사라면 좀 모르는 말이 갑자기 나와도 괜찮다고 봐야 할까?\n",
        "\n",
        "## 중간과정에서 가사를 참조하지 않음\n",
        "중간과정에서 이 model이 어떤 문장을 내뱉을 것인지는 전혀 생각하지 않았다. lms노드에서는 이런 model의 평가방법 중 하나로 '그럴싸한 문장을 만드는지 사람이 보고 평가'하는 것을 제시하고 있었지만 전혀 고려하지 않았따. 문득 처음 작사가 모델을 만들 거라고 했을 때 멜로디가 있냐고 물었던 것이 기억난다. 만약 가사와 멜로디가 붙어있다면, 아주 간단하게는 음의 높이와 길이만 주어져 있다고 해도 단어를 다시 음절로 분리할 필요가 있었을 것이다. 아무튼 각각의 발음과 음절의 길이에 의해 형성되는 운율감을 고려하게 됐을 것이고, 그러면 좀 더 흥미로웠겠지만 훨씬 무거운 계산이 되었을 것이다. 내가 say라고 넣자 이 모델은 'say it all say it all ,' 이라는 결과를 내놓았다. 어떤 장르에라도 나올 법한 가사이다. 그래서 템포나 멜로디 같은 것이 떠오르지는 않았다. 만약 특정 장르의 가사만을 뽑아서 넣었다면 좀 달랐겠지?"
      ],
      "metadata": {
        "id": "uBBxBiYRB8kn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AIFFEL LMS EXP6 Lyricist.ipynb",
      "provenance": [],
      "mount_file_id": "19Wd67yNwuBbOcHQ5FyVi0jH4X2n95_fE",
      "authorship_tag": "ABX9TyPinVnd/v9LCQpWcgflERYF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}