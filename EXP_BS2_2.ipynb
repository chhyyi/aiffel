{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LMS EXP_BS2 1_3kindOfIris_B.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPGy3ukiLMosxcQ7k7+AoZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chhyyi/aiffel/blob/main/EXP_BS2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LMS EXP1\n",
        "\n",
        "## 1.3. scikit learn의 데이터 살펴보기\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "print(dir(iris)) #dir()은 객체가 가진 변수와 메서드 나열\n",
        "print(iris.keys()) #정보의 종류. 밑에 호출해서 보는 것들\n",
        "iris_data = iris.data\n",
        "print(iris_data.shape) #150개 데이터가 4개의 feature 가짐\n",
        "\n",
        "iris.target #사용할 타겟 정보. 꽃의 종류가 0,1,2\n",
        "iris.target_name #iris.target의 숫자와 붓꽃의 종류 매칭\n",
        "iris.DESCR #데이터에 관한 상세설명\n",
        "iris.feature_names\n",
        "iris.filename \n",
        "```\n",
        "\n",
        "각각 확인해서 보면 좋겠네.\n",
        "\n",
        "## 1.4. 데이터 준비하기 with pandas\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "iris_df=pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "iris_df['label']=iris.target\n",
        "\n",
        "#target의 label을 알고 있는 데이터를 둘로 나눠서 정확도 평가할 것임\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2. random_state=7)\n"
      ],
      "metadata": {
        "id": "fGjlfU9RN1Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1.5. 모델 학습시키기\n",
        "\n",
        "Decision Tree - ratsgo's blog 요약\n",
        "[Decision Tree by ratsgo] (ratsgo.github.io/machine learning/2017/03/26/tree)\n",
        "새로운 단어를 이렇게 많이 쓰면 영어로도 같이 써놓으면 좋겠다. \n",
        "이 사람은 믿을 만한 사람인가?\n",
        "- homogenity 증가, impurity/uncertainty가 최소가 되도록 분기시킴\n",
        "$\\text{Entropy(A) is given as} -\\sum_{k=1}^m p_k log_2(p_k)$\n",
        "$P_k$ A영역에 속하는 레코드 가운데 k범주에 속하는 레코드의 비율\n",
        "m은 범주의 총 수\n",
        "\t+ 이 외에도 Gini Index가 많이 쓰인다는데 설명이 충분치 않다. 공식만 던져놓을 거면 공식을 왜 던져놓는 것인가?\n",
        "- 모델 학습 : 재귀적 분기(recursive partitioning)와 가지치기(pruning) 두 과정으로 나뉨\n",
        "- Full tree : 모든 terminal node의 순도가 100%인 상태. 여기서 적절히 ternminal node를 결합해주지 않으면 과적합(overfitting)할 염려가 생긴다고 한다.   \n",
        "Q) overffiting을 과적합이라고 하는 것은 적절한 번역인가?\n",
        "\n",
        "- cost function을 최소화시키도록 분기. 오분류율, 구조의 복잡도에 좌우된다.\n",
        "\n",
        "## 결론\n",
        "- 계산복잡성 대비 높은 성능\n",
        "- 변수 단위의 설명력\n",
        "> 'decision boundary가 data 축에 수직이라 특정 데이터에만 잘 작동할 가능성'\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=32) #random state Q?\n",
        "print(decision_tree._estimator_type)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VgwVJvqBV3Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-6 머신러닝 모델 평가하기\n",
        "\n",
        "y_pred에 학습된 모델에 의한 결과 저장\n",
        "```python\n",
        "y_pred = decision_tree.predict(X_test)\n",
        "y_pred\n",
        "```   \n",
        "\n",
        "acccuracy 확인\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy\n",
        "\n",
        "## 1-7 다른 모델 적용해보기\n",
        "\n",
        "### 그 전에 한 코드로 정리를 해봅니다.\n",
        "\n",
        "```python\n",
        "#(1) 필요한 모듈 import\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import Classification_report\n",
        "\n",
        "#(2) data preparation\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_label = iris.target\n",
        "\n",
        "#(3) train, test 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=7)\n",
        "\n",
        "#(4) 모델 학습 및 예측\n",
        "decision_tree = DecisionTreeClassifier(random_state=32)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "y_pred = decision_tree.predic(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "### Random Forest\n",
        "Ensenble 기법 사용. Ensenble 기법은 Tree와 같은 것을 여러 개 만들어서 그 결과를 통합해서 결론을 내리는 방식  \n",
        "Random Forest에서는 의사 결정 트리를 만드는데 쓰이는 요소를 무작위로 선정한다.\n",
        "> Feature가 30개라 했을 때 하나의 결정 트리로는 overffiting을 야기한다.\n",
        "Q) 이것은 일반적인 것인가?\n",
        "\n",
        "Random Forest 기법 sklearn.ensemble에서 사용하기\n",
        "\n",
        "```python \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state=21)\n",
        "#test size와 random state는 계속 쓰는데 뭔가?\n",
        "random_forest = RandomForestClassifier(random_state=32)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "\n",
        "print(Classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "### 다른 내장 분류모델들\n",
        "- SVM : 초평면hyperplane을 이용해서 분류를 수행하게 함\n",
        "    + Kernel Trick: 저차원의 공간을 고차원의 mapping. 데이터의 분포가 Linearly separable하지 않을 경우 데이터를 고차원으로 이동시켜 Linearly separable하게 만듬\n",
        "    + cost: Decision Boundary와 Margin의 간격 설정??  \n",
        "    + \n",
        "$\\gamma$: train data당 영향을 끼치는 범위 조정. 작으면 data당 영향을 끼치는 범위가 커져서 Decision boundary가 다 직선이 돼버린다.\n",
        "    + 이진 분류 모델의 다중 클래스 분류 모델화. one-vs.rest 또는 one-vs.all 방법을 이용. Q)뭔데 이게\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn import svm\n",
        "svm_model = svm.SVC()\n",
        "\n",
        "print(svm_model._estimator_type)\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "```\n",
        "\n",
        "## Stochastic Gradient Descent Classifier (SGDclassifier)\n",
        "\n",
        "- 배치 크기가 1인 경사하강법 알고리즘. 하나의 데이터 포인트를 이용하여 각 단계의 예측 경사를 계산해서 최소값을 찾는다??\n",
        "    + SGDClassifier에서는 무작위로 균일하게 선택한 하나의 데이터포인트 이용\n",
        "    + '배치란? 경사하강법에서 배치는 단일 반복에서 기울기를 계산하는데 사용하는 예(data)의 총 개수. Gradient Descent에서의 배치는 전체 데이터 셋'\n",
        "- 배치에 적은 예(data)를 무작위로 선택하면 노이즈는 있겠지만 훨씬 적은 데이터 세트로 중요한 평균값 추정 가능. 확률적 경사하강법은 이 아이디어를 확장, 배치 크기1을 사용.\n",
        "- 노이즈가 커서 최저점을 찾지 못할 수도 있음. 이를 극복하기 위해 '미니 배치 SGD'사용. 무작위로 선택한 10~1000개의 예를 미니 배치로 사용.\n",
        "\n",
        "## logistic regression\n",
        "- a.k.a. sofmax regression. 회귀지만 실제로는 분류. 가장 널리 알려진 선형 분류 알고리즘을 이용\n",
        "- 소프트맥스 함수에 대한 설명. 뭔소리야?\n",
        "> 클래스가 N개일 때, N차원의 벡터가 각 클래스가 정답일 확률을 표현하도록 정규화를 해주는 함수. 위의 그림은 4차원의 벡터를 입력으로 받아 3개의 클래스를 예측하는 경우의 소프트맥스 회귀의 동작 과정을 보여준다.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "print(logistic_mode._estimator_type)\n",
        "```\n",
        "Q나)logistic_mode._estimator_type 이건 sklearn에 다 있는 것일까?\n"
      ],
      "metadata": {
        "id": "NE6QHOsAWqI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8. 다양하게 평가해보기\n",
        "손글씨 자료로 알아보는 정확도의 치명적 문제\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "#1797개의 데이터가 있고, 각 64 len array 로 이루어져 있습니다.\n",
        "\n",
        "#이미지 출력 방법\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imgshow(digits.data[0].reshape(8,8),cmap='gray')\n",
        "plt.axis('off')\n",
        "pt.show()\n",
        "```\n",
        "\n",
        "### 3인지 아닌지 맞추는 문제에서 보는 asymmetric data에서의 정확도 문제\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=32)\n",
        "\n",
        "X_train, x_test, y_train, y_test = train_test_split(digits_data, new_label, test_size=0.2, random_state=3)\n",
        "decision_tree = DecisionTreeClassifier(random_state=32)\n",
        "decision_tree.fit(X_train,y_train)\n",
        "y_pred = decision_tree.predict(x_test)\n",
        "\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "```\n",
        "\n",
        "이거 정확도가 96%가 돼버렸다. 왜 이렇지? LMS에선 93퍼센트랬는데. 그리고 fake_pred는 88%가 나왔다.(LMS에선 93%) 아무튼 이게 unbalanced data라고 한다.\n",
        "\n",
        "## 1-9 오차 행렬\n",
        "\n",
        "[What is Confusion Matrix and Advanced Classification Metrices?](https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html)  \n",
        "\n",
        "||predict_true|predict_false||\n",
        "|----|----|----|----|\n",
        "|real_true|True Positive|False Negative|recall $\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$|\n",
        "|real_false|FP|TN|Specificity $\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$|\n",
        "||Precision $\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$|Negative Predictive Value|accuracy|\n",
        "\n",
        "where 'Negative Predictive Value' is $\\frac{\\text{TN}}{\\text{TN}+\\text{FN}}$,   accuracy is $\\frac{TP+TN}{TP+FP+FN+TN}$\n",
        "\n",
        "F1 score는 recall과 precision의 조화평균. $2 \\times \\frac{R \\times P}{R + P}$\n",
        "\n",
        "### Confusion Matrix & classification report\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "print(confusion_matrx(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "## 1-10 데이터가 달라도 문제없어요?\n",
        "\n"
      ],
      "metadata": {
        "id": "LEJJp25O1vvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine, load_breast_cancer\n",
        "#print(help(load_wine))\n",
        "print(load_wine().DESCR)\n",
        "#print(help(load_breast_cancer))"
      ],
      "metadata": {
        "id": "zT72R-77CIit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b673fa6-8142-49fc-8bab-00f234e0e761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178 (50 in each of three classes)\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  Comparison of Classifiers in High Dimensional Settings, \n",
            "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Technometrics). \n",
            "\n",
            "  The data was used with many others for comparing various \n",
            "  classifiers. The classes are separable, though only RDA \n",
            "  has achieved 100% correct classification. \n",
            "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "  (All results using the leave-one-out technique) \n",
            "\n",
            "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Journal of Chemometrics).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tg5JV2HKRsIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.11~ to be continued\n",
        "it will be LMS_EXP1-B.ipynb\n",
        "\n",
        "breast_cancer, 손글씨, 그리고 한 개 더 아무튼 classification하기에 적절한 데이터셋 세 개에 대해\n",
        "공통적으로 다음 과정을 거치라고 한다.\n",
        "1. 필요한 모듈을 import. sklearn에서 model_selection.train_test_split, metrics.classification_report  \n",
        "2. 데이터 준비. load_dataname()\n",
        "3. 데이터 이해 feature, label, target name, DESCR\n",
        "4. train, test 분리\n",
        "5. decision tree, random forest, SVM, SGD Classifier, Logistic regression 적용\n",
        "6. 모델 평가"
      ],
      "metadata": {
        "id": "2kxlBfIoSqYU"
      }
    }
  ]
}